{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da6057f",
   "metadata": {},
   "source": [
    "# 1.Import and Configuration(导入与配置)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb956875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    ")\n",
    "import random\n",
    "from kan import KAN  # pip install kan \n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import plotly.graph_objs as go\n",
    "from glob import glob\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'ITG', 'BTW', 'CTR', 'ETR', 'S', 'L', 'WA', 'WFR', 'WWR', 'DN',\n",
    "    'WN', 'DS', 'WS', 'H', 'V', 'a', 'b', 'OH', 'IH', 'CN', 'CA'\n",
    "]\n",
    "INPUT_FEATURES = FEATURE_COLUMNS\n",
    "#FEATURES_LAYER1 = ['ITG', 'BTW', 'CTR', 'ETR', 'S', 'H', 'V', 'a', 'b']\n",
    "#FEATURES_LAYER2 = ['L', 'WA', 'WFR', 'WWR', 'DN', 'WN', 'DS', 'WS', 'OH', 'IH', 'CN', 'CA']\n",
    "FEATURES_LAYER1 = ['ITG', 'BTW', 'CTR', 'ETR', 'WA', 'WFR', 'WWR', 'WS' , 'CA']\n",
    "FEATURES_LAYER2 = ['S', 'H', 'V', 'a', 'b' , 'L', 'DN', 'WN', 'DS',  'OH', 'IH', 'CN']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4b391",
   "metadata": {},
   "source": [
    "# 2.Data processing & related functions(数据处理相关函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09827981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedStandardScaler:\n",
    "    def __init__(self):\n",
    "        self.group_scalers = {}\n",
    "    def fit(self, X, groups):\n",
    "        self.group_scalers = {}\n",
    "        groups = pd.Series(groups)\n",
    "        for g in groups.unique():\n",
    "            scaler = StandardScaler()\n",
    "            mask = (groups == g)\n",
    "            scaler.fit(X.loc[mask])\n",
    "            self.group_scalers[g] = scaler\n",
    "        return self\n",
    "    def transform(self, X, groups):\n",
    "        X_out = X.copy()\n",
    "        groups = pd.Series(groups)\n",
    "        for g, scaler in self.group_scalers.items():\n",
    "            mask = (groups == g)\n",
    "            X_out.loc[mask] = scaler.transform(X.loc[mask])\n",
    "        return X_out\n",
    "\n",
    "def read_data(folder_path):\n",
    "    data_list = []\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "    if not file_names:\n",
    "        raise FileNotFoundError(f\"No .xlsx files in {folder_path}\")\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            if sheet_name == 'data':\n",
    "                continue\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            df['source'] = f\"{file_name}_{sheet_name}\"\n",
    "            data_list.append(df)\n",
    "    if not data_list:\n",
    "        raise ValueError(\"No valid sheets loaded in read_data\")\n",
    "    return pd.concat(data_list, ignore_index=True)\n",
    "\n",
    "def read_edges(folder_path):\n",
    "    edge_list = []\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            if sheet_name == 'data':\n",
    "                continue\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            df['source'] = f\"{file_name}_{sheet_name}\"\n",
    "            edge_list.append(df)\n",
    "    return pd.concat(edge_list, ignore_index=True)\n",
    "\n",
    "def get_root_layer_nodes_for_sheet(node_data, node_ids):\n",
    "    df = node_data[node_data['global_id'].isin(node_ids)]\n",
    "    for tag in ['EN', 'DT', 'LT']:\n",
    "        lst = df[df['Class'] == tag]['global_id'].tolist()\n",
    "        if lst:\n",
    "            return lst\n",
    "    return [node_ids[0]]\n",
    "\n",
    "def process_data(node_data, edge_data):\n",
    "    node_data = node_data.copy()\n",
    "    node_data['global_id'] = range(len(node_data))\n",
    "    id_map = {(row['source'], row['ID']): row['global_id'] for _, row in node_data.iterrows()}\n",
    "    all_input_cols = INPUT_FEATURES\n",
    "    idx_layer1 = [all_input_cols.index(col) for col in FEATURES_LAYER1]\n",
    "    idx_layer2 = [all_input_cols.index(col) for col in FEATURES_LAYER2]\n",
    "    group_col = 'Class'\n",
    "    group_series = node_data[group_col].astype(str)\n",
    "    group_features = ['BTW', 'CTR']\n",
    "    other_features = [col for col in INPUT_FEATURES if col not in group_features]\n",
    "    input_scaler = StandardScaler()\n",
    "    X_other = input_scaler.fit_transform(node_data[other_features].fillna(0))\n",
    "    group_scaler_BTW = GroupedStandardScaler()\n",
    "    group_scaler_BTW.fit(node_data[['BTW']].fillna(0), group_series)\n",
    "    group_scaler_CTR = GroupedStandardScaler()\n",
    "    group_scaler_CTR.fit(node_data[['CTR']].fillna(0), group_series)\n",
    "    X_BTW = group_scaler_BTW.transform(node_data[['BTW']].fillna(0), group_series).values\n",
    "    X_CTR = group_scaler_CTR.transform(node_data[['CTR']].fillna(0), group_series).values\n",
    "    X = np.zeros((len(node_data), len(all_input_cols)))\n",
    "    for i, col in enumerate(all_input_cols):\n",
    "        if col == 'BTW':\n",
    "            X[:, i] = X_BTW[:, 0]\n",
    "        elif col == 'CTR':\n",
    "            X[:, i] = X_CTR[:, 0]\n",
    "        else:\n",
    "            X[:, i] = X_other[:, other_features.index(col)]\n",
    "    edge_list = []\n",
    "    for _, row in edge_data.iterrows():\n",
    "        src_key = (row['source'], row['StartPointID'])\n",
    "        dst_key = (row['source'], row['EndPointID'])\n",
    "        if src_key in id_map and dst_key in id_map:\n",
    "            edge_list.append([id_map[src_key], id_map[dst_key]])\n",
    "    edge_index = torch.LongTensor(edge_list).t().contiguous()\n",
    "    return {\n",
    "        'x': torch.FloatTensor(X),\n",
    "        'edge_index': edge_index,\n",
    "        'idx_layer1': idx_layer1,\n",
    "        'idx_layer2': idx_layer2,\n",
    "        'node_data': node_data,\n",
    "        'id_map': id_map\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a931951",
   "metadata": {},
   "source": [
    "# 3.Model structure definition(模型结构定义)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549be404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DualMLP_EdgeClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        idx_layer1,\n",
    "        idx_layer2,\n",
    "        mlp_dim=32,\n",
    "        hidden_dim=64,\n",
    "        dropout=0.2,\n",
    "        heads=2,    \n",
    "        concat=True,\n",
    "        residual=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.idx_layer1 = idx_layer1\n",
    "        self.idx_layer2 = idx_layer2\n",
    "\n",
    "        # Projection\n",
    "        self.proj1 = nn.Linear(len(idx_layer1), mlp_dim)\n",
    "        self.proj2 = nn.Linear(len(idx_layer2), mlp_dim)\n",
    "\n",
    "        # Group MLPs (for each feature group independently)\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(mlp_dim, mlp_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim * 2, mlp_dim)\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(mlp_dim, mlp_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim * 2, mlp_dim)\n",
    "        )\n",
    "\n",
    "        out_dim = hidden_dim * heads if concat else hidden_dim  \n",
    "\n",
    "        # Dim align\n",
    "        self.mlp_align1 = nn.Linear(mlp_dim, out_dim)\n",
    "        self.mlp_align2 = nn.Linear(mlp_dim, out_dim)\n",
    "        \n",
    "        # Feature fusion gating\n",
    "        self.fusion_gate = nn.Sequential(\n",
    "            nn.Linear(out_dim * 4, out_dim * 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = residual\n",
    "        self.classifier = None\n",
    "        self.out1_dim = out_dim\n",
    "        self.out2_dim = out_dim\n",
    "\n",
    "    def build_classifier(self, feats_dim, device):\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feats_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.18),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_pairs):\n",
    "        device = x.device\n",
    "        idx_layer1 = self.idx_layer1\n",
    "        idx_layer2 = self.idx_layer2\n",
    "        x1 = x[:, idx_layer1]\n",
    "        x2 = x[:, idx_layer2]\n",
    "\n",
    "        # Two sets of feature independent MLP transformation\n",
    "        x1_proj = self.proj1(x1)\n",
    "        h1_mlp = self.mlp1(x1_proj)\n",
    "        h1_aligned = self.mlp_align1(h1_mlp)\n",
    "        h1_mlp = self.dropout(h1_aligned)\n",
    "\n",
    "        x2_proj = self.proj2(x2)\n",
    "        h2_mlp = self.mlp2(x2_proj)\n",
    "        h2_aligned = self.mlp_align2(h2_mlp)\n",
    "        h2_mlp = self.dropout(h2_aligned)\n",
    "\n",
    "        feats_all = torch.cat([h1_mlp, h1_mlp, h2_mlp, h2_mlp], dim=1)  # [N, out_dim*4]\n",
    "        gate = self.fusion_gate(feats_all)\n",
    "        gate1, gate2, gate3, gate4 = torch.chunk(gate, 4, dim=1)\n",
    "        fused = (gate1 * h1_mlp) + (gate2 * h1_mlp) + (gate3 * h2_mlp) + (gate4 * h2_mlp)\n",
    "\n",
    "        # Take the concatenation of the two endpoints in the participating edge pairs as the representation of each \"edge\"\n",
    "        edge_feat_i = fused[edge_pairs[0]]  # [n_edges, out_dim]\n",
    "        edge_feat_j = fused[edge_pairs[1]]  # [n_edges, out_dim]\n",
    "        edge_feat = torch.cat([edge_feat_i, edge_feat_j], dim=1) # [n_edges, out_dim * 2]\n",
    "        \n",
    "        if self.classifier is None:\n",
    "            self.build_classifier(edge_feat.size(1), device)\n",
    "\n",
    "        logits = self.classifier(edge_feat)\n",
    "        return logits  # [n_edges, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d822f5e",
   "metadata": {},
   "source": [
    "# 4.Evaluation, auxiliary, and visualization functions(评估、辅助和可视化函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f726e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_tree_layers_priority(edges, node_ids, node_data):\n",
    "    root_layer_nodes = get_root_layer_nodes_for_sheet(node_data, node_ids)\n",
    "    edge_set = set((min(a, b), max(a, b)) for a, b in edges)\n",
    "    layers = {nid: 0 for nid in root_layer_nodes}\n",
    "    q = deque(list(root_layer_nodes))\n",
    "    used = set(root_layer_nodes)\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in node_ids:\n",
    "            if v not in used and (min(u, v), max(u, v)) in edge_set:\n",
    "                layers[v] = layers[u] + 1\n",
    "                used.add(v)\n",
    "                q.append(v)\n",
    "    return layers\n",
    "\n",
    "def edge_evaluate(logits, labels):\n",
    "    prob_edge = F.softmax(logits, dim=1)[:, 1]\n",
    "    pred_label = (prob_edge > 0.5).cpu().numpy().astype(np.int64)\n",
    "    y_true = labels.cpu().numpy()\n",
    "    acc = accuracy_score(y_true, pred_label)\n",
    "    f1 = f1_score(y_true, pred_label)\n",
    "    recall = recall_score(y_true, pred_label)\n",
    "    precision = precision_score(y_true, pred_label)\n",
    "    roc_auc = roc_auc_score(y_true, prob_edge.detach().cpu().numpy()) if np.sum(y_true) > 0 and np.sum(1 - y_true) > 0 else 0.0\n",
    "    loss = F.cross_entropy(logits, labels.long()).item()\n",
    "    cm = confusion_matrix(y_true, pred_label)\n",
    "    tn, fp, fn, tp = (cm.ravel().tolist() if cm.size == 4 else (0, 0, 0, 0))\n",
    "    return {\n",
    "        'loss': loss, 'acc': acc, 'f1': f1, 'recall': recall, 'precision': precision, 'roc_auc': roc_auc,\n",
    "        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "        'prob': prob_edge.detach().cpu().numpy(), 'pred_label': pred_label, 'y_true': y_true\n",
    "    }\n",
    "\n",
    "def plot_metrics_curve(history, tag, outdir):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    epochs = np.arange(len(history['train']))\n",
    "    for metric in ['loss', 'f1', 'roc_auc', 'recall', 'acc']:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        for phase in ['train', 'val', 'test']:\n",
    "            plt.plot(epochs, [h[metric] for h in history[phase]], label=phase)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{tag} {metric} Curve')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(outdir, f\"{tag}_curve_{metric}.png\"), dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "def infer_sheet_prob_matrix(model, features, edge_index, node_ids, batchsize=2048):\n",
    "    pairs = []\n",
    "    for i in node_ids:\n",
    "        for j in node_ids:\n",
    "            if i < j:\n",
    "                pairs.append((i, j))\n",
    "    pairs = np.array(pairs).T\n",
    "    with torch.no_grad():\n",
    "        probs = []\n",
    "        for start in range(0, pairs.shape[1], batchsize):\n",
    "            batch_pairs = pairs[:, start:start + batchsize]\n",
    "            logits = model(features, edge_index, torch.LongTensor(batch_pairs).to(features.device))\n",
    "            proba = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
    "            probs.append(proba)\n",
    "        all_probs = np.concatenate(probs)\n",
    "    N = len(node_ids)\n",
    "    mat = np.zeros((N, N))\n",
    "    idxmap = {nid: idx for idx, nid in enumerate(node_ids)}\n",
    "    k = 0\n",
    "    for a, b in zip(pairs[0], pairs[1]):\n",
    "        ia, ib = idxmap[a], idxmap[b]\n",
    "        mat[ia, ib] = all_probs[k]\n",
    "        mat[ib, ia] = all_probs[k]\n",
    "        k += 1\n",
    "    np.fill_diagonal(mat, 0)\n",
    "    return mat, idxmap\n",
    "\n",
    "def plot_depth_graph(df, edge_list, node_depths, id_col, name_col, save_path, color='#f7941d', title=\"结构图\"):\n",
    "    nodes = list(df['global_id'].values)\n",
    "    names = {row['global_id']: str(row[name_col]) for _, row in df.iterrows()}\n",
    "    depths = [node_depths[n] for n in nodes if n in node_depths]\n",
    "    max_depth = max(depths) if depths else 1\n",
    "    depth_layers = {}\n",
    "    for n in nodes:\n",
    "        d = node_depths.get(n, 0)\n",
    "        depth_layers.setdefault(d, []).append(n)\n",
    "    ygap = 2.7 if len(depth_layers) <= 10 else 1.5\n",
    "    pos = {}\n",
    "    for d, layer in depth_layers.items():\n",
    "        x_gap = 1.1 if len(layer) <= 10 else 0.65\n",
    "        for i, nid in enumerate(sorted(layer, key=lambda v: names[v])):\n",
    "            pos[nid] = (i * x_gap, d * ygap)\n",
    "    plt.figure(figsize=(max(12, 0.55 * len(nodes)), (max_depth + 3) * 1.3), dpi=600)\n",
    "    for n in nodes:\n",
    "        x, y = pos[n]\n",
    "        plt.scatter(x, y, s=1000, color='deepskyblue', edgecolors='k', zorder=20)\n",
    "        plt.text(x, y, names[n], fontsize=10, ha='center', va='center', color='black', weight='bold', zorder=25)\n",
    "    for (a, b) in edge_list:\n",
    "        x1, y1 = pos[a]; x2, y2 = pos[b]\n",
    "        plt.plot([x1, x2], [y1, y2], c=color, lw=3, alpha=0.76, zorder=10)\n",
    "    for d in range(max_depth + 1):\n",
    "        plt.axhline(y=d * ygap, color='gray', linewidth=0.7, linestyle='dashed', alpha=0.17)\n",
    "        plt.text(-2, d * ygap, f'Depth: {d}', fontsize=13, color='k', weight='bold', va='center')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "def get_true_layers(node_ids, edges, node_data):\n",
    "    roots = get_root_layer_nodes_for_sheet(node_data, node_ids)\n",
    "    adj = defaultdict(list)\n",
    "    for u, v in edges:\n",
    "        adj[u].append(v)\n",
    "        adj[v].append(u)\n",
    "    depths = {n: 0 for n in roots}\n",
    "    q = deque(roots)\n",
    "    visited = set(roots)\n",
    "    while q:\n",
    "        u = q.popleft()\n",
    "        for v in adj[u]:\n",
    "            if v not in visited:\n",
    "                depths[v] = depths[u] + 1\n",
    "                q.append(v)\n",
    "                visited.add(v)\n",
    "    max_depth = max(depths.values()) if depths else 0\n",
    "    layers = [[] for _ in range(max_depth + 1)]\n",
    "    for n, d in depths.items():\n",
    "        layers[d].append(n)\n",
    "    return layers\n",
    "\n",
    "def pred_structure_deep_layers(prob_mat, node_ids, node_data, layer_sizes, root_nodes):\n",
    "    N = len(node_ids)\n",
    "    idxmap = {nid: i for i, nid in enumerate(node_ids)}\n",
    "    used = set(root_nodes)\n",
    "    alloc = []\n",
    "    alloc.append(list(root_nodes))\n",
    "    left = set(node_ids) - set(root_nodes)\n",
    "    cur_layer = list(root_nodes)\n",
    "    depth = 1\n",
    "    while depth < len(layer_sizes):\n",
    "        scores = []\n",
    "        for nid in left:\n",
    "            s = np.mean([prob_mat[idxmap[nid], idxmap[u]] for u in cur_layer])\n",
    "            scores.append((s, nid))\n",
    "        scores.sort(reverse=True)\n",
    "        need_num = max(1, len(layer_sizes[depth]))\n",
    "        layer_nodes = [nid for _, nid in scores[:need_num]]\n",
    "        alloc.append(layer_nodes)\n",
    "        left -= set(layer_nodes)\n",
    "        cur_layer = layer_nodes\n",
    "        depth += 1\n",
    "    if left:\n",
    "        alloc[-1].extend(left)\n",
    "    return alloc\n",
    "\n",
    "def make_pred_edges_by_layers(prob_mat, alloc_layers, pred_prob_thresh=0.5):\n",
    "    layers = alloc_layers\n",
    "    idxmap = {}\n",
    "    node_ids = []\n",
    "    for layer in layers:\n",
    "        node_ids.extend(layer)\n",
    "    for idx, nid in enumerate(node_ids):\n",
    "        idxmap[nid] = idx\n",
    "    pred_edges = []\n",
    "    for dep in range(1, len(layers)):\n",
    "        for v in layers[dep]:\n",
    "            bestu = None\n",
    "            bestscore = -1\n",
    "            for u in layers[dep - 1]:\n",
    "                s = prob_mat[idxmap[u], idxmap[v]]\n",
    "                if s > bestscore:\n",
    "                    bestscore = s\n",
    "                    bestu = u\n",
    "            if bestu is not None:\n",
    "                pred_edges.append(tuple(sorted([v, bestu])))\n",
    "    return pred_edges\n",
    "\n",
    "def structure_layer_loss(pred_edges, node_ids, node_data, real_layers):\n",
    "    pred_layers = bfs_tree_layers_priority(pred_edges, node_ids, node_data)\n",
    "    real_layers_map = {}\n",
    "    for d, l in enumerate(real_layers):\n",
    "        for n in l:\n",
    "            real_layers_map[n] = d\n",
    "    loss = 0\n",
    "    for n in node_ids:\n",
    "        loss += (pred_layers.get(n, 999) - real_layers_map.get(n, 999)) ** 2\n",
    "    return loss\n",
    "\n",
    "def depth_sa_optimize(prob_mat, node_ids, node_data, real_layers, pred_edges):\n",
    "    best_edges = pred_edges[:]\n",
    "    best_score = structure_layer_loss(best_edges, node_ids, node_data, real_layers)\n",
    "    cur_edges = pred_edges[:]\n",
    "    for epoch in range(3000):\n",
    "        nonroot = []\n",
    "        for i, layer in enumerate(real_layers):\n",
    "            if i == 0: continue\n",
    "            nonroot.extend(layer)\n",
    "        if not nonroot: break\n",
    "        tgt = random.choice(nonroot)\n",
    "        candidate_p = []\n",
    "        prev_layer = []\n",
    "        for li, ly in enumerate(real_layers):\n",
    "            if tgt in ly and li > 0:\n",
    "                prev_layer = real_layers[li - 1]\n",
    "                break\n",
    "        for u in prev_layer:\n",
    "            if tuple(sorted([u, tgt])) not in cur_edges:\n",
    "                candidate_p.append(u)\n",
    "        if candidate_p:\n",
    "            u_new = random.choice(candidate_p)\n",
    "            edges_new = [e for e in cur_edges if tgt not in e] + [tuple(sorted([u_new, tgt]))]\n",
    "        else:\n",
    "            continue\n",
    "        score_new = structure_layer_loss(edges_new, node_ids, node_data, real_layers)\n",
    "        dE = best_score - score_new\n",
    "        if score_new < best_score or random.random() < np.exp(dE / 2.4):\n",
    "            cur_edges = edges_new\n",
    "            if score_new < best_score:\n",
    "                best_edges = edges_new\n",
    "                best_score = score_new\n",
    "    return best_edges\n",
    "\n",
    "def save_edges(edge_list, nd, outcsv):\n",
    "    id2name = dict(zip(nd['global_id'], nd['Name']))\n",
    "    edge_rows = []\n",
    "    for a, b in edge_list:\n",
    "        edge_rows.append({'src_id': a, 'src_name': id2name.get(a, ''),'dst_id': b, 'dst_name': id2name.get(b, '')})\n",
    "    pd.DataFrame(edge_rows).to_csv(outcsv, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2233ee",
   "metadata": {},
   "source": [
    "# 5.Data loading and feature processing(数据加载与特征处理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500298b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点数: 2722 特征数: 21\n",
      "边索引 shape: torch.Size([2, 3023])\n"
     ]
    }
   ],
   "source": [
    "# Note the folder path\n",
    "node_data = read_data('nodedata')\n",
    "edge_data = read_edges('edgedata')\n",
    "data_info = process_data(node_data, edge_data)\n",
    "features = data_info['x'].to(device)\n",
    "edge_index = data_info['edge_index'].to(device)\n",
    "idx_layer1 = data_info['idx_layer1']\n",
    "idx_layer2 = data_info['idx_layer2']\n",
    "node_data_pd = data_info['node_data']\n",
    "id_map = data_info['id_map']\n",
    "num_nodes = features.shape[0]\n",
    "print(\"节点数:\", features.shape[0], \"特征数:\", features.shape[1])\n",
    "print(\"边索引 shape:\", edge_index.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e43d1f",
   "metadata": {},
   "source": [
    "# 6.Construction and division of positive and negative samples(正负样本构建与划分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b86554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2, 3627) (3627,)\n",
      "Val: (2, 1209) (1209,)\n",
      "Test: (2, 1210) (1210,)\n"
     ]
    }
   ],
   "source": [
    "pos_edges = edge_index.cpu().numpy().T\n",
    "exists = set(tuple(sorted([i, j])) for i, j in pos_edges)\n",
    "neg_edges = []\n",
    "while len(neg_edges) < len(pos_edges):\n",
    "    i, j = np.random.randint(0, num_nodes, size=2)\n",
    "    if i == j: continue\n",
    "    if tuple(sorted([i, j])) in exists: continue\n",
    "    neg_edges.append([i, j])\n",
    "    exists.add(tuple(sorted([i, j])))\n",
    "neg_edges = np.array(neg_edges)\n",
    "all_edges = np.concatenate([pos_edges, neg_edges], axis=0)\n",
    "labels = np.array([1] * len(pos_edges) + [0] * len(neg_edges))\n",
    "idx = np.random.permutation(len(labels))\n",
    "all_edges = all_edges[idx]; labels = labels[idx]\n",
    "n = len(labels)\n",
    "n_train = int(0.6 * n); n_val = int(0.2 * n)\n",
    "train_edges = all_edges[:n_train].T; train_labels = labels[:n_train]\n",
    "val_edges = all_edges[n_train:n_train + n_val].T; val_labels = labels[n_train:n_train + n_val]\n",
    "test_edges = all_edges[n_train + n_val:].T; test_labels = labels[n_train + n_val:]\n",
    "print(\"Train:\", train_edges.shape, train_labels.shape)\n",
    "print(\"Val:\", val_edges.shape, val_labels.shape)\n",
    "print(\"Test:\", test_edges.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cfa544",
   "metadata": {},
   "source": [
    "# 7.Training main loop(训练主循环)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4caf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Start Model Training with Dynamic LR (ReduceLROnPlateau) ====\n",
      "\n",
      "Epoch 001/200 | Loss: 0.6917/0.6923/0.6915 | EdgeF1: 0.564/0.561/0.584 | EdgeAP: 0.547/0.529/0.573 | Recall: 0.594/0.577/0.599 | ROC:    0.574/0.551/0.590\n",
      "\n",
      "Epoch 002/200 | Loss: 0.6897/0.6905/0.6897 | EdgeF1: 0.630/0.618/0.642 | EdgeAP: 0.620/0.586/0.624 | Recall: 0.657/0.643/0.673 | ROC:    0.662/0.625/0.666\n",
      "\n",
      "Epoch 003/200 | Loss: 0.6881/0.6884/0.6888 | EdgeF1: 0.663/0.652/0.644 | EdgeAP: 0.654/0.632/0.629 | Recall: 0.690/0.659/0.670 | ROC:    0.706/0.686/0.680\n",
      "\n",
      "Epoch 004/200 | Loss: 0.6864/0.6878/0.6863 | EdgeF1: 0.686/0.668/0.679 | EdgeAP: 0.680/0.651/0.669 | Recall: 0.709/0.674/0.700 | ROC:    0.727/0.690/0.735\n",
      "\n",
      "Epoch 005/200 | Loss: 0.6837/0.6851/0.6843 | EdgeF1: 0.699/0.696/0.696 | EdgeAP: 0.698/0.676/0.689 | Recall: 0.713/0.710/0.711 | ROC:    0.761/0.727/0.745\n",
      "\n",
      "Epoch 006/200 | Loss: 0.6819/0.6833/0.6829 | EdgeF1: 0.701/0.702/0.705 | EdgeAP: 0.700/0.687/0.691 | Recall: 0.713/0.708/0.738 | ROC:    0.760/0.734/0.746\n",
      "\n",
      "Epoch 007/200 | Loss: 0.6796/0.6812/0.6805 | EdgeF1: 0.716/0.721/0.705 | EdgeAP: 0.714/0.697/0.696 | Recall: 0.730/0.748/0.724 | ROC:    0.771/0.745/0.761\n",
      "\n",
      "Epoch 008/200 | Loss: 0.6768/0.6785/0.6781 | EdgeF1: 0.719/0.713/0.727 | EdgeAP: 0.714/0.692/0.712 | Recall: 0.746/0.731/0.766 | ROC:    0.773/0.746/0.767\n",
      "\n",
      "Epoch 009/200 | Loss: 0.6739/0.6766/0.6748 | EdgeF1: 0.728/0.732/0.740 | EdgeAP: 0.722/0.711/0.728 | Recall: 0.756/0.756/0.771 | ROC:    0.778/0.756/0.769\n",
      "\n",
      "Epoch 010/200 | Loss: 0.6704/0.6736/0.6722 | EdgeF1: 0.732/0.730/0.735 | EdgeAP: 0.725/0.709/0.721 | Recall: 0.760/0.756/0.774 | ROC:    0.784/0.757/0.774\n",
      "\n",
      "Epoch 011/200 | Loss: 0.6665/0.6701/0.6682 | EdgeF1: 0.740/0.730/0.744 | EdgeAP: 0.730/0.706/0.730 | Recall: 0.779/0.764/0.782 | ROC:    0.788/0.753/0.776\n",
      "\n",
      "Epoch 012/200 | Loss: 0.6622/0.6666/0.6636 | EdgeF1: 0.738/0.739/0.747 | EdgeAP: 0.728/0.715/0.735 | Recall: 0.780/0.772/0.784 | ROC:    0.791/0.758/0.782\n",
      "\n",
      "Epoch 013/200 | Loss: 0.6587/0.6638/0.6598 | EdgeF1: 0.731/0.740/0.739 | EdgeAP: 0.719/0.715/0.724 | Recall: 0.776/0.775/0.781 | ROC:    0.788/0.758/0.785\n",
      "\n",
      "Epoch 014/200 | Loss: 0.6516/0.6591/0.6550 | EdgeF1: 0.740/0.741/0.748 | EdgeAP: 0.726/0.712/0.731 | Recall: 0.792/0.788/0.795 | ROC:    0.792/0.760/0.787\n",
      "\n",
      "Epoch 015/200 | Loss: 0.6486/0.6565/0.6486 | EdgeF1: 0.747/0.741/0.755 | EdgeAP: 0.734/0.712/0.736 | Recall: 0.800/0.791/0.814 | ROC:    0.790/0.754/0.790\n",
      "\n",
      "Epoch 016/200 | Loss: 0.6429/0.6495/0.6447 | EdgeF1: 0.750/0.756/0.758 | EdgeAP: 0.736/0.725/0.740 | Recall: 0.804/0.818/0.812 | ROC:    0.793/0.766/0.787\n",
      "\n",
      "Epoch 017/200 | Loss: 0.6371/0.6435/0.6416 | EdgeF1: 0.747/0.755/0.759 | EdgeAP: 0.730/0.725/0.740 | Recall: 0.813/0.811/0.817 | ROC:    0.789/0.765/0.782\n",
      "\n",
      "Epoch 018/200 | Loss: 0.6300/0.6392/0.6322 | EdgeF1: 0.754/0.755/0.755 | EdgeAP: 0.739/0.725/0.736 | Recall: 0.814/0.811/0.812 | ROC:    0.795/0.765/0.790\n",
      "\n",
      "Epoch 019/200 | Loss: 0.6256/0.6342/0.6286 | EdgeF1: 0.753/0.754/0.753 | EdgeAP: 0.736/0.725/0.732 | Recall: 0.816/0.810/0.817 | ROC:    0.791/0.768/0.787\n",
      "\n",
      "Epoch 020/200 | Loss: 0.6190/0.6293/0.6206 | EdgeF1: 0.755/0.761/0.764 | EdgeAP: 0.738/0.730/0.742 | Recall: 0.820/0.821/0.832 | ROC:    0.795/0.767/0.791\n",
      "\n",
      "Epoch 021/200 | Loss: 0.6111/0.6206/0.6139 | EdgeF1: 0.756/0.763/0.763 | EdgeAP: 0.741/0.734/0.744 | Recall: 0.817/0.819/0.825 | ROC:    0.800/0.774/0.795\n",
      "\n",
      "Epoch 022/200 | Loss: 0.6060/0.6193/0.6078 | EdgeF1: 0.763/0.764/0.768 | EdgeAP: 0.746/0.734/0.750 | Recall: 0.828/0.823/0.828 | ROC:    0.799/0.769/0.799\n",
      "\n",
      "Epoch 023/200 | Loss: 0.6008/0.6124/0.6060 | EdgeF1: 0.762/0.760/0.769 | EdgeAP: 0.746/0.730/0.750 | Recall: 0.826/0.816/0.832 | ROC:    0.800/0.779/0.792\n",
      "\n",
      "Epoch 024/200 | Loss: 0.5919/0.6074/0.5998 | EdgeF1: 0.760/0.777/0.770 | EdgeAP: 0.745/0.749/0.752 | Recall: 0.819/0.838/0.827 | ROC:    0.803/0.780/0.796\n",
      "\n",
      "Epoch 025/200 | Loss: 0.5862/0.6090/0.5900 | EdgeF1: 0.763/0.762/0.772 | EdgeAP: 0.749/0.733/0.755 | Recall: 0.823/0.821/0.828 | ROC:    0.806/0.775/0.802\n",
      "\n",
      "Epoch 026/200 | Loss: 0.5774/0.5996/0.5852 | EdgeF1: 0.766/0.759/0.766 | EdgeAP: 0.754/0.733/0.750 | Recall: 0.818/0.805/0.815 | ROC:    0.812/0.779/0.802\n",
      "\n",
      "Epoch 027/200 | Loss: 0.5720/0.5926/0.5822 | EdgeF1: 0.759/0.769/0.764 | EdgeAP: 0.748/0.747/0.749 | Recall: 0.806/0.805/0.810 | ROC:    0.811/0.785/0.800\n",
      "\n",
      "Epoch 028/200 | Loss: 0.5643/0.5889/0.5736 | EdgeF1: 0.765/0.754/0.764 | EdgeAP: 0.756/0.733/0.753 | Recall: 0.806/0.786/0.797 | ROC:    0.815/0.783/0.806\n",
      "\n",
      "Epoch 029/200 | Loss: 0.5579/0.5715/0.5725 | EdgeF1: 0.760/0.765/0.756 | EdgeAP: 0.754/0.746/0.750 | Recall: 0.790/0.791/0.772 | ROC:    0.816/0.795/0.803\n",
      "\n",
      "Epoch 030/200 | Loss: 0.5558/0.5722/0.5589 | EdgeF1: 0.761/0.764/0.769 | EdgeAP: 0.758/0.746/0.761 | Recall: 0.780/0.786/0.792 | ROC:    0.814/0.791/0.815\n",
      "\n",
      "Epoch 031/200 | Loss: 0.5453/0.5748/0.5516 | EdgeF1: 0.758/0.754/0.758 | EdgeAP: 0.758/0.739/0.754 | Recall: 0.768/0.765/0.771 | ROC:    0.820/0.786/0.816\n",
      "\n",
      "Epoch 032/200 | Loss: 0.5412/0.5662/0.5442 | EdgeF1: 0.756/0.746/0.756 | EdgeAP: 0.758/0.734/0.755 | Recall: 0.761/0.748/0.761 | ROC:    0.819/0.791/0.819\n",
      "\n",
      "Epoch 033/200 | Loss: 0.5357/0.5676/0.5460 | EdgeF1: 0.754/0.756/0.761 | EdgeAP: 0.758/0.744/0.760 | Recall: 0.754/0.758/0.761 | ROC:    0.822/0.795/0.816\n",
      "\n",
      "Epoch 034/200 | Loss: 0.5291/0.5620/0.5412 | EdgeF1: 0.754/0.748/0.754 | EdgeAP: 0.760/0.737/0.755 | Recall: 0.749/0.748/0.749 | ROC:    0.826/0.799/0.820\n",
      "\n",
      "Epoch 035/200 | Loss: 0.5275/0.5550/0.5390 | EdgeF1: 0.745/0.742/0.761 | EdgeAP: 0.752/0.734/0.763 | Recall: 0.735/0.734/0.756 | ROC:    0.825/0.802/0.818\n",
      "\n",
      "Epoch 036/200 | Loss: 0.5218/0.5473/0.5404 | EdgeF1: 0.752/0.749/0.763 | EdgeAP: 0.757/0.739/0.764 | Recall: 0.749/0.745/0.761 | ROC:    0.828/0.804/0.815\n",
      "\n",
      "Epoch 037/200 | Loss: 0.5175/0.5546/0.5287 | EdgeF1: 0.753/0.745/0.759 | EdgeAP: 0.758/0.736/0.759 | Recall: 0.750/0.739/0.759 | ROC:    0.832/0.802/0.822\n",
      "\n",
      "Epoch 038/200 | Loss: 0.5194/0.5488/0.5362 | EdgeF1: 0.757/0.747/0.748 | EdgeAP: 0.761/0.736/0.750 | Recall: 0.756/0.748/0.743 | ROC:    0.828/0.806/0.818\n",
      "\n",
      "Epoch 039/200 | Loss: 0.5199/0.5598/0.5235 | EdgeF1: 0.762/0.753/0.770 | EdgeAP: 0.763/0.739/0.767 | Recall: 0.772/0.761/0.779 | ROC:    0.826/0.799/0.826\n",
      "\n",
      "Epoch 040/200 | Loss: 0.5080/0.5459/0.5252 | EdgeF1: 0.768/0.762/0.773 | EdgeAP: 0.767/0.749/0.766 | Recall: 0.786/0.772/0.794 | ROC:    0.832/0.809/0.824\n",
      "\n",
      "Epoch 041/200 | Loss: 0.5117/0.5429/0.5214 | EdgeF1: 0.770/0.767/0.783 | EdgeAP: 0.769/0.752/0.776 | Recall: 0.786/0.783/0.807 | ROC:    0.832/0.810/0.827\n",
      "\n",
      "Epoch 042/200 | Loss: 0.5065/0.5334/0.5264 | EdgeF1: 0.770/0.769/0.777 | EdgeAP: 0.766/0.752/0.768 | Recall: 0.793/0.791/0.807 | ROC:    0.834/0.815/0.824\n",
      "\n",
      "Epoch 043/200 | Loss: 0.5087/0.5389/0.5253 | EdgeF1: 0.772/0.760/0.770 | EdgeAP: 0.767/0.743/0.760 | Recall: 0.800/0.781/0.802 | ROC:    0.833/0.812/0.823\n",
      "\n",
      "Epoch 044/200 | Loss: 0.5058/0.5372/0.5216 | EdgeF1: 0.775/0.774/0.773 | EdgeAP: 0.770/0.758/0.764 | Recall: 0.806/0.797/0.799 | ROC:    0.834/0.811/0.826\n",
      "\n",
      "Epoch 045/200 | Loss: 0.5040/0.5344/0.5263 | EdgeF1: 0.776/0.769/0.777 | EdgeAP: 0.771/0.752/0.765 | Recall: 0.809/0.792/0.818 | ROC:    0.833/0.812/0.822\n",
      "\n",
      "Epoch 046/200 | Loss: 0.5009/0.5345/0.5073 | EdgeF1: 0.773/0.768/0.776 | EdgeAP: 0.768/0.750/0.768 | Recall: 0.803/0.792/0.805 | ROC:    0.835/0.810/0.831\n",
      "\n",
      "Epoch 047/200 | Loss: 0.5004/0.5318/0.5186 | EdgeF1: 0.773/0.758/0.774 | EdgeAP: 0.768/0.741/0.765 | Recall: 0.800/0.777/0.800 | ROC:    0.835/0.813/0.827\n",
      "\n",
      "Epoch 048/200 | Loss: 0.4997/0.5258/0.5221 | EdgeF1: 0.773/0.763/0.771 | EdgeAP: 0.766/0.745/0.763 | Recall: 0.806/0.784/0.797 | ROC:    0.833/0.815/0.821\n",
      "\n",
      "Epoch 049/200 | Loss: 0.4975/0.5213/0.5110 | EdgeF1: 0.779/0.773/0.774 | EdgeAP: 0.775/0.757/0.769 | Recall: 0.807/0.796/0.790 | ROC:    0.836/0.816/0.828\n",
      "\n",
      "Epoch 050/200 | Loss: 0.4917/0.5150/0.5146 | EdgeF1: 0.776/0.769/0.764 | EdgeAP: 0.772/0.752/0.758 | Recall: 0.801/0.789/0.784 | ROC:    0.840/0.817/0.824\n",
      "\n",
      "Epoch 051/200 | Loss: 0.4919/0.5231/0.5106 | EdgeF1: 0.776/0.763/0.775 | EdgeAP: 0.771/0.748/0.769 | Recall: 0.803/0.780/0.797 | ROC:    0.840/0.815/0.826\n",
      "\n",
      "Epoch 052/200 | Loss: 0.4965/0.5184/0.5070 | EdgeF1: 0.774/0.764/0.781 | EdgeAP: 0.769/0.746/0.770 | Recall: 0.805/0.786/0.817 | ROC:    0.833/0.820/0.826\n",
      "\n",
      "Epoch 053/200 | Loss: 0.4902/0.5188/0.5053 | EdgeF1: 0.782/0.780/0.784 | EdgeAP: 0.776/0.760/0.775 | Recall: 0.816/0.815/0.814 | ROC:    0.839/0.813/0.829\n",
      "\n",
      "Epoch 054/200 | Loss: 0.4911/0.5176/0.5042 | EdgeF1: 0.780/0.778/0.773 | EdgeAP: 0.773/0.758/0.762 | Recall: 0.820/0.813/0.807 | ROC:    0.838/0.816/0.827\n",
      "\n",
      "Epoch 055/200 | Loss: 0.4877/0.5232/0.5012 | EdgeF1: 0.781/0.765/0.784 | EdgeAP: 0.774/0.745/0.771 | Recall: 0.820/0.792/0.828 | ROC:    0.840/0.812/0.829\n",
      "\n",
      "Epoch 056/200 | Loss: 0.4934/0.5197/0.5028 | EdgeF1: 0.784/0.782/0.786 | EdgeAP: 0.775/0.762/0.774 | Recall: 0.829/0.821/0.825 | ROC:    0.834/0.813/0.826\n",
      "\n",
      "Epoch 057/200 | Loss: 0.4892/0.5139/0.5066 | EdgeF1: 0.785/0.779/0.780 | EdgeAP: 0.777/0.759/0.765 | Recall: 0.826/0.811/0.830 | ROC:    0.840/0.819/0.822\n",
      "\n",
      "Epoch 058/200 | Loss: 0.4867/0.5163/0.4977 | EdgeF1: 0.788/0.782/0.782 | EdgeAP: 0.779/0.763/0.770 | Recall: 0.833/0.816/0.823 | ROC:    0.840/0.817/0.832\n",
      "Epoch 00058: reducing learning rate of group 0 to 2.0000e-04.\n",
      "\n",
      "Epoch 059/200 | Loss: 0.4890/0.5116/0.4970 | EdgeF1: 0.784/0.772/0.779 | EdgeAP: 0.776/0.750/0.765 | Recall: 0.826/0.808/0.827 | ROC:    0.838/0.818/0.833\n",
      "\n",
      "Epoch 060/200 | Loss: 0.4815/0.5210/0.4983 | EdgeF1: 0.788/0.767/0.788 | EdgeAP: 0.780/0.748/0.775 | Recall: 0.831/0.796/0.833 | ROC:    0.846/0.814/0.832\n",
      "\n",
      "Epoch 061/200 | Loss: 0.4810/0.5197/0.5012 | EdgeF1: 0.785/0.769/0.783 | EdgeAP: 0.776/0.747/0.769 | Recall: 0.831/0.807/0.828 | ROC:    0.845/0.810/0.831\n",
      "\n",
      "Epoch 062/200 | Loss: 0.4877/0.5089/0.5036 | EdgeF1: 0.785/0.775/0.781 | EdgeAP: 0.776/0.751/0.767 | Recall: 0.833/0.819/0.830 | ROC:    0.840/0.822/0.825\n",
      "\n",
      "Epoch 063/200 | Loss: 0.4854/0.5112/0.5082 | EdgeF1: 0.782/0.781/0.777 | EdgeAP: 0.773/0.761/0.766 | Recall: 0.828/0.818/0.812 | ROC:    0.840/0.819/0.824\n",
      "\n",
      "Epoch 064/200 | Loss: 0.4821/0.5138/0.5049 | EdgeF1: 0.791/0.778/0.781 | EdgeAP: 0.782/0.758/0.771 | Recall: 0.835/0.813/0.817 | ROC:    0.842/0.818/0.825\n",
      "\n",
      "Epoch 065/200 | Loss: 0.4843/0.5105/0.4974 | EdgeF1: 0.790/0.777/0.782 | EdgeAP: 0.781/0.759/0.768 | Recall: 0.834/0.802/0.832 | ROC:    0.841/0.822/0.830\n",
      "\n",
      "Epoch 066/200 | Loss: 0.4837/0.5128/0.5030 | EdgeF1: 0.785/0.765/0.775 | EdgeAP: 0.777/0.745/0.763 | Recall: 0.830/0.796/0.817 | ROC:    0.843/0.819/0.829\n",
      "\n",
      "Epoch 067/200 | Loss: 0.4844/0.5063/0.5006 | EdgeF1: 0.787/0.788/0.780 | EdgeAP: 0.778/0.768/0.768 | Recall: 0.829/0.826/0.820 | ROC:    0.843/0.823/0.831\n",
      "\n",
      "Epoch 068/200 | Loss: 0.4881/0.5159/0.4990 | EdgeF1: 0.783/0.775/0.773 | EdgeAP: 0.775/0.755/0.763 | Recall: 0.824/0.807/0.809 | ROC:    0.837/0.818/0.833\n",
      "\n",
      "Epoch 069/200 | Loss: 0.4855/0.5092/0.5041 | EdgeF1: 0.782/0.781/0.779 | EdgeAP: 0.774/0.759/0.769 | Recall: 0.823/0.823/0.815 | ROC:    0.841/0.820/0.829\n",
      "\n",
      "Epoch 070/200 | Loss: 0.4804/0.5191/0.4949 | EdgeF1: 0.793/0.777/0.787 | EdgeAP: 0.785/0.755/0.776 | Recall: 0.836/0.816/0.827 | ROC:    0.845/0.814/0.834\n",
      "\n",
      "Epoch 071/200 | Loss: 0.4846/0.5204/0.5038 | EdgeF1: 0.779/0.768/0.768 | EdgeAP: 0.772/0.748/0.759 | Recall: 0.814/0.800/0.797 | ROC:    0.841/0.816/0.827\n",
      "\n",
      "Epoch 072/200 | Loss: 0.4845/0.5143/0.5051 | EdgeF1: 0.783/0.775/0.779 | EdgeAP: 0.776/0.755/0.767 | Recall: 0.821/0.808/0.822 | ROC:    0.842/0.816/0.829\n",
      "\n",
      "Epoch 073/200 | Loss: 0.4854/0.5180/0.4969 | EdgeF1: 0.785/0.774/0.783 | EdgeAP: 0.776/0.754/0.773 | Recall: 0.830/0.807/0.820 | ROC:    0.840/0.815/0.831\n",
      "Epoch 00073: reducing learning rate of group 0 to 4.0000e-05.\n",
      "\n",
      "Epoch 074/200 | Loss: 0.4816/0.5140/0.4959 | EdgeF1: 0.785/0.779/0.786 | EdgeAP: 0.777/0.760/0.774 | Recall: 0.826/0.810/0.828 | ROC:    0.843/0.821/0.834\n",
      "\n",
      "Epoch 075/200 | Loss: 0.4830/0.5116/0.4994 | EdgeF1: 0.791/0.779/0.782 | EdgeAP: 0.782/0.759/0.771 | Recall: 0.838/0.813/0.822 | ROC:    0.842/0.818/0.833\n",
      "\n",
      "Epoch 076/200 | Loss: 0.4827/0.5158/0.4973 | EdgeF1: 0.788/0.779/0.780 | EdgeAP: 0.780/0.758/0.769 | Recall: 0.831/0.818/0.820 | ROC:    0.843/0.817/0.830\n",
      "\n",
      "Epoch 077/200 | Loss: 0.4827/0.5082/0.5010 | EdgeF1: 0.785/0.777/0.783 | EdgeAP: 0.777/0.758/0.771 | Recall: 0.828/0.808/0.827 | ROC:    0.842/0.824/0.830\n",
      "\n",
      "Epoch 078/200 | Loss: 0.4816/0.5188/0.5017 | EdgeF1: 0.792/0.776/0.780 | EdgeAP: 0.783/0.756/0.769 | Recall: 0.837/0.810/0.818 | ROC:    0.843/0.817/0.828\n",
      "\n",
      "Epoch 079/200 | Loss: 0.4830/0.5090/0.5010 | EdgeF1: 0.785/0.786/0.780 | EdgeAP: 0.777/0.768/0.768 | Recall: 0.825/0.818/0.822 | ROC:    0.843/0.822/0.828\n",
      "\n",
      "Epoch 080/200 | Loss: 0.4893/0.5193/0.4904 | EdgeF1: 0.790/0.777/0.779 | EdgeAP: 0.782/0.757/0.767 | Recall: 0.834/0.811/0.820 | ROC:    0.837/0.816/0.836\n",
      "\n",
      "Epoch 081/200 | Loss: 0.4822/0.5237/0.4962 | EdgeF1: 0.789/0.766/0.785 | EdgeAP: 0.781/0.745/0.776 | Recall: 0.830/0.799/0.818 | ROC:    0.843/0.812/0.834\n",
      "\n",
      "Epoch 082/200 | Loss: 0.4805/0.5087/0.5063 | EdgeF1: 0.788/0.780/0.783 | EdgeAP: 0.779/0.759/0.772 | Recall: 0.834/0.818/0.823 | ROC:    0.844/0.824/0.830\n",
      "\n",
      "Epoch 083/200 | Loss: 0.4832/0.5166/0.4964 | EdgeF1: 0.786/0.770/0.787 | EdgeAP: 0.777/0.750/0.775 | Recall: 0.832/0.802/0.828 | ROC:    0.842/0.816/0.831\n",
      "\n",
      "Epoch 084/200 | Loss: 0.4838/0.5230/0.4926 | EdgeF1: 0.791/0.774/0.785 | EdgeAP: 0.783/0.754/0.774 | Recall: 0.833/0.810/0.823 | ROC:    0.843/0.813/0.835\n",
      "\n",
      "Epoch 085/200 | Loss: 0.4895/0.5116/0.4962 | EdgeF1: 0.781/0.786/0.774 | EdgeAP: 0.774/0.766/0.764 | Recall: 0.819/0.823/0.810 | ROC:    0.838/0.818/0.833\n",
      "\n",
      "Epoch 086/200 | Loss: 0.4800/0.5159/0.5023 | EdgeF1: 0.789/0.782/0.781 | EdgeAP: 0.780/0.761/0.769 | Recall: 0.835/0.821/0.825 | ROC:    0.843/0.816/0.828\n",
      "\n",
      "Epoch 087/200 | Loss: 0.4817/0.5133/0.5016 | EdgeF1: 0.785/0.780/0.781 | EdgeAP: 0.776/0.759/0.768 | Recall: 0.829/0.818/0.828 | ROC:    0.844/0.816/0.827\n",
      "\n",
      "Epoch 088/200 | Loss: 0.4848/0.5142/0.4987 | EdgeF1: 0.787/0.784/0.783 | EdgeAP: 0.778/0.763/0.769 | Recall: 0.830/0.821/0.830 | ROC:    0.841/0.817/0.830\n",
      "Epoch 00088: reducing learning rate of group 0 to 8.0000e-06.\n",
      "\n",
      "Epoch 089/200 | Loss: 0.4804/0.5117/0.4866 | EdgeF1: 0.788/0.781/0.782 | EdgeAP: 0.779/0.760/0.768 | Recall: 0.833/0.819/0.830 | ROC:    0.842/0.818/0.841\n",
      "\n",
      "Epoch 090/200 | Loss: 0.4850/0.5219/0.4990 | EdgeF1: 0.787/0.770/0.777 | EdgeAP: 0.778/0.749/0.764 | Recall: 0.835/0.805/0.820 | ROC:    0.840/0.809/0.831\n",
      "\n",
      "Epoch 091/200 | Loss: 0.4833/0.5178/0.5006 | EdgeF1: 0.790/0.781/0.771 | EdgeAP: 0.781/0.761/0.760 | Recall: 0.838/0.815/0.810 | ROC:    0.841/0.817/0.830\n",
      "\n",
      "Epoch 092/200 | Loss: 0.4821/0.5153/0.4968 | EdgeF1: 0.791/0.774/0.777 | EdgeAP: 0.783/0.754/0.765 | Recall: 0.834/0.810/0.815 | ROC:    0.843/0.815/0.834\n",
      "\n",
      "Epoch 093/200 | Loss: 0.4873/0.5104/0.4961 | EdgeF1: 0.784/0.783/0.785 | EdgeAP: 0.775/0.763/0.774 | Recall: 0.828/0.818/0.823 | ROC:    0.838/0.818/0.836\n",
      "\n",
      "Epoch 094/200 | Loss: 0.4808/0.5156/0.4972 | EdgeF1: 0.785/0.777/0.778 | EdgeAP: 0.776/0.757/0.766 | Recall: 0.831/0.811/0.820 | ROC:    0.845/0.815/0.832\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.6000e-06.\n",
      "\n",
      "Epoch 095/200 | Loss: 0.4854/0.5129/0.5014 | EdgeF1: 0.785/0.777/0.780 | EdgeAP: 0.776/0.755/0.769 | Recall: 0.831/0.816/0.815 | ROC:    0.839/0.816/0.828\n",
      "\n",
      "Epoch 096/200 | Loss: 0.4826/0.5135/0.5032 | EdgeF1: 0.792/0.775/0.776 | EdgeAP: 0.784/0.755/0.763 | Recall: 0.836/0.807/0.822 | ROC:    0.842/0.820/0.827\n",
      "\n",
      "Epoch 097/200 | Loss: 0.4875/0.5090/0.4977 | EdgeF1: 0.781/0.786/0.776 | EdgeAP: 0.772/0.766/0.763 | Recall: 0.825/0.824/0.820 | ROC:    0.839/0.821/0.833\n",
      "\n",
      "Epoch 098/200 | Loss: 0.4850/0.5128/0.5002 | EdgeF1: 0.788/0.780/0.780 | EdgeAP: 0.778/0.762/0.767 | Recall: 0.839/0.810/0.823 | ROC:    0.840/0.819/0.833\n",
      "\n",
      "Epoch 099/200 | Loss: 0.4804/0.5146/0.4970 | EdgeF1: 0.785/0.773/0.783 | EdgeAP: 0.776/0.755/0.771 | Recall: 0.828/0.799/0.823 | ROC:    0.844/0.819/0.834\n",
      "\n",
      "Epoch 100/200 | Loss: 0.4869/0.5072/0.5024 | EdgeF1: 0.788/0.779/0.781 | EdgeAP: 0.779/0.760/0.768 | Recall: 0.830/0.808/0.827 | ROC:    0.839/0.822/0.829\n",
      "Epoch 00100: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Epoch 101/200 | Loss: 0.4801/0.5151/0.5019 | EdgeF1: 0.786/0.776/0.783 | EdgeAP: 0.779/0.754/0.770 | Recall: 0.826/0.813/0.827 | ROC:    0.844/0.816/0.832\n",
      "\n",
      "Epoch 102/200 | Loss: 0.4843/0.5162/0.5000 | EdgeF1: 0.789/0.767/0.777 | EdgeAP: 0.781/0.745/0.764 | Recall: 0.833/0.805/0.818 | ROC:    0.842/0.814/0.830\n",
      "\n",
      "Epoch 103/200 | Loss: 0.4850/0.5178/0.4961 | EdgeF1: 0.790/0.773/0.781 | EdgeAP: 0.782/0.756/0.769 | Recall: 0.834/0.794/0.822 | ROC:    0.841/0.813/0.832\n",
      "\n",
      "Epoch 104/200 | Loss: 0.4851/0.5219/0.5012 | EdgeF1: 0.783/0.777/0.772 | EdgeAP: 0.774/0.757/0.761 | Recall: 0.826/0.810/0.805 | ROC:    0.841/0.813/0.831\n",
      "\n",
      "Epoch 105/200 | Loss: 0.4801/0.5101/0.5014 | EdgeF1: 0.785/0.780/0.775 | EdgeAP: 0.777/0.759/0.762 | Recall: 0.826/0.819/0.817 | ROC:    0.844/0.820/0.830\n",
      "\n",
      "Epoch 106/200 | Loss: 0.4799/0.5125/0.4977 | EdgeF1: 0.791/0.784/0.781 | EdgeAP: 0.782/0.763/0.769 | Recall: 0.840/0.824/0.820 | ROC:    0.845/0.819/0.833\n",
      "\n",
      "Epoch 107/200 | Loss: 0.4844/0.5103/0.5035 | EdgeF1: 0.783/0.774/0.780 | EdgeAP: 0.774/0.755/0.769 | Recall: 0.829/0.805/0.815 | ROC:    0.841/0.821/0.828\n",
      "\n",
      "Epoch 108/200 | Loss: 0.4804/0.5137/0.4948 | EdgeF1: 0.788/0.773/0.780 | EdgeAP: 0.779/0.754/0.766 | Recall: 0.835/0.802/0.828 | ROC:    0.845/0.819/0.834\n",
      "\n",
      "Epoch 109/200 | Loss: 0.4844/0.5172/0.4998 | EdgeF1: 0.785/0.772/0.779 | EdgeAP: 0.777/0.752/0.767 | Recall: 0.830/0.807/0.818 | ROC:    0.840/0.816/0.832\n",
      "\n",
      "Epoch 110/200 | Loss: 0.4809/0.5195/0.5009 | EdgeF1: 0.790/0.774/0.779 | EdgeAP: 0.782/0.751/0.767 | Recall: 0.835/0.815/0.822 | ROC:    0.843/0.813/0.829\n",
      "\n",
      "Epoch 111/200 | Loss: 0.4813/0.5156/0.5109 | EdgeF1: 0.785/0.775/0.773 | EdgeAP: 0.776/0.756/0.759 | Recall: 0.829/0.805/0.822 | ROC:    0.842/0.818/0.823\n",
      "\n",
      "Epoch 112/200 | Loss: 0.4842/0.5075/0.4964 | EdgeF1: 0.788/0.778/0.782 | EdgeAP: 0.780/0.758/0.767 | Recall: 0.830/0.810/0.833 | ROC:    0.842/0.823/0.833\n",
      "\n",
      "Epoch 113/200 | Loss: 0.4812/0.5110/0.5075 | EdgeF1: 0.783/0.778/0.771 | EdgeAP: 0.775/0.757/0.760 | Recall: 0.825/0.818/0.807 | ROC:    0.843/0.821/0.821\n",
      "\n",
      "Epoch 114/200 | Loss: 0.4806/0.5222/0.4923 | EdgeF1: 0.788/0.770/0.786 | EdgeAP: 0.781/0.751/0.774 | Recall: 0.826/0.797/0.830 | ROC:    0.843/0.812/0.837\n",
      "\n",
      "Epoch 115/200 | Loss: 0.4813/0.5097/0.5075 | EdgeF1: 0.791/0.780/0.776 | EdgeAP: 0.781/0.758/0.763 | Recall: 0.839/0.819/0.820 | ROC:    0.844/0.820/0.826\n",
      "\n",
      "Epoch 116/200 | Loss: 0.4838/0.5201/0.5037 | EdgeF1: 0.784/0.775/0.772 | EdgeAP: 0.778/0.755/0.760 | Recall: 0.819/0.810/0.810 | ROC:    0.842/0.811/0.829\n",
      "\n",
      "Epoch 117/200 | Loss: 0.4773/0.5162/0.5010 | EdgeF1: 0.785/0.776/0.780 | EdgeAP: 0.778/0.755/0.768 | Recall: 0.824/0.811/0.820 | ROC:    0.847/0.815/0.830\n",
      "\n",
      "Epoch 118/200 | Loss: 0.4874/0.5109/0.4997 | EdgeF1: 0.788/0.776/0.778 | EdgeAP: 0.779/0.756/0.765 | Recall: 0.833/0.810/0.820 | ROC:    0.840/0.821/0.831\n",
      "\n",
      "Epoch 119/200 | Loss: 0.4804/0.5121/0.4975 | EdgeF1: 0.793/0.779/0.780 | EdgeAP: 0.785/0.758/0.768 | Recall: 0.839/0.818/0.820 | ROC:    0.843/0.819/0.833\n",
      "\n",
      "Epoch 120/200 | Loss: 0.4817/0.5095/0.5043 | EdgeF1: 0.787/0.772/0.775 | EdgeAP: 0.779/0.749/0.761 | Recall: 0.830/0.815/0.820 | ROC:    0.842/0.824/0.828\n",
      "\n",
      "Epoch 121/200 | Loss: 0.4825/0.5086/0.5048 | EdgeF1: 0.786/0.781/0.783 | EdgeAP: 0.778/0.761/0.772 | Recall: 0.830/0.816/0.820 | ROC:    0.842/0.823/0.826\n",
      "\n",
      "Epoch 122/200 | Loss: 0.4820/0.5142/0.5095 | EdgeF1: 0.780/0.771/0.771 | EdgeAP: 0.772/0.749/0.758 | Recall: 0.819/0.810/0.815 | ROC:    0.845/0.817/0.823\n",
      "\n",
      "Epoch 123/200 | Loss: 0.4809/0.5180/0.4983 | EdgeF1: 0.782/0.770/0.780 | EdgeAP: 0.774/0.750/0.770 | Recall: 0.823/0.802/0.812 | ROC:    0.846/0.815/0.835\n",
      "\n",
      "Epoch 124/200 | Loss: 0.4845/0.5127/0.4948 | EdgeF1: 0.789/0.782/0.790 | EdgeAP: 0.780/0.762/0.778 | Recall: 0.835/0.818/0.833 | ROC:    0.840/0.820/0.831\n",
      "\n",
      "Epoch 125/200 | Loss: 0.4848/0.5190/0.5072 | EdgeF1: 0.792/0.779/0.784 | EdgeAP: 0.782/0.759/0.772 | Recall: 0.840/0.813/0.825 | ROC:    0.841/0.816/0.825\n",
      "\n",
      "Epoch 126/200 | Loss: 0.4839/0.5126/0.4936 | EdgeF1: 0.787/0.786/0.780 | EdgeAP: 0.779/0.766/0.769 | Recall: 0.829/0.826/0.817 | ROC:    0.841/0.820/0.836\n",
      "\n",
      "Epoch 127/200 | Loss: 0.4818/0.5072/0.4972 | EdgeF1: 0.787/0.782/0.781 | EdgeAP: 0.781/0.762/0.769 | Recall: 0.825/0.819/0.822 | ROC:    0.844/0.823/0.833\n",
      "\n",
      "Epoch 128/200 | Loss: 0.4833/0.5149/0.5073 | EdgeF1: 0.786/0.775/0.772 | EdgeAP: 0.778/0.755/0.760 | Recall: 0.826/0.807/0.814 | ROC:    0.841/0.816/0.828\n",
      "\n",
      "Epoch 129/200 | Loss: 0.4804/0.5177/0.4977 | EdgeF1: 0.792/0.776/0.785 | EdgeAP: 0.785/0.757/0.772 | Recall: 0.833/0.807/0.833 | ROC:    0.845/0.814/0.831\n",
      "\n",
      "Epoch 130/200 | Loss: 0.4853/0.5068/0.5073 | EdgeF1: 0.788/0.770/0.788 | EdgeAP: 0.779/0.749/0.777 | Recall: 0.835/0.802/0.828 | ROC:    0.838/0.823/0.825\n",
      "\n",
      "Epoch 131/200 | Loss: 0.4837/0.5154/0.4962 | EdgeF1: 0.783/0.768/0.780 | EdgeAP: 0.774/0.748/0.768 | Recall: 0.826/0.799/0.822 | ROC:    0.842/0.817/0.834\n",
      "\n",
      "Epoch 132/200 | Loss: 0.4843/0.5184/0.5067 | EdgeF1: 0.789/0.770/0.774 | EdgeAP: 0.781/0.749/0.760 | Recall: 0.834/0.807/0.820 | ROC:    0.841/0.815/0.825\n",
      "\n",
      "Epoch 133/200 | Loss: 0.4828/0.5091/0.5009 | EdgeF1: 0.789/0.770/0.778 | EdgeAP: 0.779/0.749/0.766 | Recall: 0.835/0.802/0.818 | ROC:    0.843/0.821/0.830\n",
      "\n",
      "Epoch 134/200 | Loss: 0.4819/0.5150/0.5032 | EdgeF1: 0.793/0.782/0.774 | EdgeAP: 0.785/0.761/0.761 | Recall: 0.839/0.821/0.817 | ROC:    0.842/0.819/0.828\n",
      "\n",
      "Epoch 135/200 | Loss: 0.4822/0.5161/0.4973 | EdgeF1: 0.789/0.772/0.790 | EdgeAP: 0.780/0.751/0.779 | Recall: 0.838/0.807/0.830 | ROC:    0.842/0.816/0.831\n",
      "\n",
      "Epoch 136/200 | Loss: 0.4830/0.5140/0.4985 | EdgeF1: 0.789/0.776/0.779 | EdgeAP: 0.781/0.756/0.768 | Recall: 0.834/0.810/0.817 | ROC:    0.840/0.816/0.830\n",
      "\n",
      "Epoch 137/200 | Loss: 0.4836/0.5134/0.5004 | EdgeF1: 0.785/0.786/0.784 | EdgeAP: 0.778/0.766/0.770 | Recall: 0.826/0.823/0.832 | ROC:    0.842/0.819/0.831\n",
      "\n",
      "Epoch 138/200 | Loss: 0.4855/0.5136/0.4921 | EdgeF1: 0.784/0.772/0.774 | EdgeAP: 0.776/0.750/0.764 | Recall: 0.827/0.808/0.805 | ROC:    0.840/0.818/0.836\n",
      "\n",
      "Epoch 139/200 | Loss: 0.4843/0.5167/0.5077 | EdgeF1: 0.791/0.776/0.780 | EdgeAP: 0.782/0.754/0.767 | Recall: 0.834/0.818/0.823 | ROC:    0.842/0.812/0.824\n",
      "\n",
      "Epoch 140/200 | Loss: 0.4826/0.5134/0.4945 | EdgeF1: 0.785/0.782/0.782 | EdgeAP: 0.776/0.764/0.771 | Recall: 0.828/0.808/0.822 | ROC:    0.842/0.818/0.835\n",
      "\n",
      "Epoch 141/200 | Loss: 0.4852/0.5104/0.5033 | EdgeF1: 0.788/0.779/0.776 | EdgeAP: 0.780/0.757/0.762 | Recall: 0.831/0.819/0.822 | ROC:    0.840/0.820/0.828\n",
      "\n",
      "Epoch 142/200 | Loss: 0.4800/0.5216/0.5008 | EdgeF1: 0.792/0.769/0.771 | EdgeAP: 0.785/0.747/0.761 | Recall: 0.832/0.805/0.802 | ROC:    0.845/0.812/0.831\n",
      "\n",
      "Epoch 143/200 | Loss: 0.4799/0.5092/0.5023 | EdgeF1: 0.786/0.789/0.777 | EdgeAP: 0.778/0.769/0.765 | Recall: 0.828/0.827/0.817 | ROC:    0.845/0.819/0.831\n",
      "\n",
      "Epoch 144/200 | Loss: 0.4858/0.5186/0.4993 | EdgeF1: 0.783/0.777/0.780 | EdgeAP: 0.774/0.755/0.767 | Recall: 0.824/0.816/0.825 | ROC:    0.839/0.814/0.832\n",
      "\n",
      "Epoch 145/200 | Loss: 0.4846/0.5179/0.4986 | EdgeF1: 0.788/0.774/0.776 | EdgeAP: 0.780/0.753/0.764 | Recall: 0.832/0.811/0.817 | ROC:    0.842/0.815/0.828\n",
      "\n",
      "Epoch 146/200 | Loss: 0.4822/0.5105/0.5108 | EdgeF1: 0.790/0.783/0.784 | EdgeAP: 0.781/0.762/0.772 | Recall: 0.837/0.824/0.825 | ROC:    0.844/0.824/0.820\n",
      "\n",
      "Epoch 147/200 | Loss: 0.4873/0.5160/0.4932 | EdgeF1: 0.783/0.772/0.782 | EdgeAP: 0.775/0.754/0.770 | Recall: 0.825/0.797/0.825 | ROC:    0.841/0.819/0.836\n",
      "\n",
      "Epoch 148/200 | Loss: 0.4805/0.5161/0.4956 | EdgeF1: 0.788/0.775/0.791 | EdgeAP: 0.780/0.757/0.779 | Recall: 0.829/0.803/0.833 | ROC:    0.845/0.818/0.832\n",
      "\n",
      "Epoch 149/200 | Loss: 0.4810/0.5039/0.4966 | EdgeF1: 0.786/0.786/0.787 | EdgeAP: 0.778/0.767/0.774 | Recall: 0.828/0.821/0.830 | ROC:    0.844/0.823/0.833\n",
      "\n",
      "Epoch 150/200 | Loss: 0.4819/0.5182/0.5029 | EdgeF1: 0.790/0.780/0.779 | EdgeAP: 0.783/0.758/0.769 | Recall: 0.828/0.821/0.814 | ROC:    0.843/0.813/0.830\n",
      "\n",
      "Epoch 151/200 | Loss: 0.4784/0.5181/0.5084 | EdgeF1: 0.793/0.781/0.777 | EdgeAP: 0.785/0.761/0.764 | Recall: 0.835/0.818/0.818 | ROC:    0.845/0.811/0.822\n",
      "\n",
      "Epoch 152/200 | Loss: 0.4853/0.5132/0.5011 | EdgeF1: 0.783/0.778/0.774 | EdgeAP: 0.774/0.758/0.763 | Recall: 0.828/0.813/0.812 | ROC:    0.840/0.818/0.831\n",
      "\n",
      "Epoch 153/200 | Loss: 0.4880/0.5186/0.5005 | EdgeF1: 0.785/0.771/0.775 | EdgeAP: 0.777/0.749/0.762 | Recall: 0.829/0.807/0.817 | ROC:    0.838/0.815/0.831\n",
      "\n",
      "Epoch 154/200 | Loss: 0.4834/0.5129/0.4962 | EdgeF1: 0.787/0.773/0.786 | EdgeAP: 0.779/0.754/0.773 | Recall: 0.832/0.800/0.832 | ROC:    0.841/0.817/0.834\n",
      "\n",
      "Epoch 155/200 | Loss: 0.4826/0.5143/0.5023 | EdgeF1: 0.790/0.770/0.776 | EdgeAP: 0.781/0.749/0.764 | Recall: 0.835/0.808/0.815 | ROC:    0.841/0.817/0.830\n",
      "\n",
      "Epoch 156/200 | Loss: 0.4816/0.5261/0.4944 | EdgeF1: 0.782/0.771/0.783 | EdgeAP: 0.773/0.751/0.770 | Recall: 0.824/0.802/0.828 | ROC:    0.844/0.809/0.834\n",
      "\n",
      "Epoch 157/200 | Loss: 0.4815/0.5101/0.4954 | EdgeF1: 0.788/0.788/0.783 | EdgeAP: 0.780/0.768/0.771 | Recall: 0.829/0.826/0.823 | ROC:    0.843/0.821/0.834\n",
      "\n",
      "Epoch 158/200 | Loss: 0.4782/0.5155/0.5023 | EdgeF1: 0.789/0.778/0.781 | EdgeAP: 0.781/0.758/0.769 | Recall: 0.831/0.811/0.822 | ROC:    0.846/0.819/0.826\n",
      "\n",
      "Epoch 159/200 | Loss: 0.4839/0.5172/0.4974 | EdgeF1: 0.784/0.782/0.780 | EdgeAP: 0.776/0.760/0.768 | Recall: 0.829/0.823/0.822 | ROC:    0.842/0.814/0.831\n",
      "\n",
      "Epoch 160/200 | Loss: 0.4825/0.5175/0.5005 | EdgeF1: 0.787/0.774/0.781 | EdgeAP: 0.779/0.751/0.769 | Recall: 0.826/0.816/0.820 | ROC:    0.843/0.817/0.829\n",
      "\n",
      "Epoch 161/200 | Loss: 0.4852/0.5132/0.5011 | EdgeF1: 0.784/0.780/0.776 | EdgeAP: 0.775/0.762/0.764 | Recall: 0.830/0.811/0.814 | ROC:    0.840/0.816/0.831\n",
      "\n",
      "Epoch 162/200 | Loss: 0.4855/0.5144/0.5008 | EdgeF1: 0.787/0.780/0.786 | EdgeAP: 0.778/0.760/0.774 | Recall: 0.833/0.813/0.828 | ROC:    0.838/0.819/0.829\n",
      "\n",
      "Epoch 163/200 | Loss: 0.4786/0.5177/0.5036 | EdgeF1: 0.785/0.776/0.774 | EdgeAP: 0.778/0.754/0.762 | Recall: 0.820/0.816/0.815 | ROC:    0.846/0.815/0.827\n",
      "\n",
      "Epoch 164/200 | Loss: 0.4804/0.5150/0.4989 | EdgeF1: 0.791/0.776/0.773 | EdgeAP: 0.783/0.756/0.760 | Recall: 0.836/0.811/0.815 | ROC:    0.845/0.818/0.829\n",
      "\n",
      "Epoch 165/200 | Loss: 0.4809/0.5204/0.5023 | EdgeF1: 0.788/0.777/0.788 | EdgeAP: 0.781/0.757/0.777 | Recall: 0.829/0.810/0.827 | ROC:    0.843/0.813/0.827\n",
      "\n",
      "Epoch 166/200 | Loss: 0.4825/0.5176/0.4975 | EdgeF1: 0.790/0.779/0.792 | EdgeAP: 0.781/0.758/0.780 | Recall: 0.837/0.815/0.835 | ROC:    0.843/0.816/0.834\n",
      "\n",
      "Epoch 167/200 | Loss: 0.4855/0.5110/0.5058 | EdgeF1: 0.788/0.777/0.774 | EdgeAP: 0.781/0.758/0.762 | Recall: 0.829/0.808/0.812 | ROC:    0.840/0.819/0.827\n",
      "\n",
      "Epoch 168/200 | Loss: 0.4818/0.5123/0.5004 | EdgeF1: 0.787/0.776/0.781 | EdgeAP: 0.780/0.757/0.770 | Recall: 0.828/0.807/0.818 | ROC:    0.844/0.819/0.829\n",
      "\n",
      "Epoch 169/200 | Loss: 0.4823/0.5136/0.4935 | EdgeF1: 0.792/0.770/0.780 | EdgeAP: 0.783/0.749/0.769 | Recall: 0.838/0.808/0.817 | ROC:    0.842/0.816/0.837\n",
      "\n",
      "Epoch 170/200 | Loss: 0.4830/0.5199/0.5033 | EdgeF1: 0.787/0.774/0.785 | EdgeAP: 0.778/0.753/0.774 | Recall: 0.832/0.813/0.825 | ROC:    0.843/0.816/0.827\n",
      "Early Stopping at epoch 170\n",
      "训练完成，最佳模型加载完毕！\n",
      "指标曲线已保存。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the random seed to ensure the experiment is reproducible\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DualMLP_EdgeClassifier(\n",
    "    input_dim=features.shape[1],\n",
    "    idx_layer1=idx_layer1,\n",
    "    idx_layer2=idx_layer2,\n",
    "    mlp_dim=32,\n",
    "    hidden_dim=64,\n",
    "    dropout=0.2,\n",
    "    heads=2,\n",
    "    concat=True,\n",
    "    residual=True\n",
    ").to(device)\n",
    "\n",
    "max_epochs = 200\n",
    "patience = 50    \n",
    "wait = 0\n",
    "best_auc = -np.inf\n",
    "best_model_state = None\n",
    "history = {'train': [], 'val': [], 'test': []}\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=3e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', patience=5, factor=0.2, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(\"\\n==== Start Model Training with Dynamic LR (ReduceLROnPlateau) ====\")\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(features, edge_index, torch.LongTensor(train_edges).to(device))\n",
    "    train_labels_tensor = torch.LongTensor(train_labels).to(device)\n",
    "    loss = F.cross_entropy(logits, train_labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate train/val/test sets\n",
    "    for phase, edge_set, lab_set in zip(\n",
    "        ['train', 'val', 'test'],\n",
    "        [train_edges, val_edges, test_edges],\n",
    "        [train_labels, val_labels, test_labels]\n",
    "    ):\n",
    "        logits_eval = model(features, edge_index, torch.LongTensor(edge_set).to(device))\n",
    "        labtensor_eval = torch.LongTensor(lab_set).to(device)\n",
    "        res = edge_evaluate(logits_eval, labtensor_eval)\n",
    "        history[phase].append(res)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1:03}/{max_epochs} |\"\n",
    "        f\" Loss: {history['train'][-1]['loss']:.4f}/{history['val'][-1]['loss']:.4f}/{history['test'][-1]['loss']:.4f}\"\n",
    "        f\" | EdgeF1: {history['train'][-1]['f1']:.3f}/{history['val'][-1]['f1']:.3f}/{history['test'][-1]['f1']:.3f}\"\n",
    "        f\" | EdgeAP: {history['train'][-1]['acc']:.3f}/{history['val'][-1]['acc']:.3f}/{history['test'][-1]['acc']:.3f}\"\n",
    "        f\" | Recall: {history['train'][-1]['recall']:.3f}/{history['val'][-1]['recall']:.3f}/{history['test'][-1]['recall']:.3f}\"\n",
    "        f\" | ROC:    {history['train'][-1]['roc_auc']:.3f}/{history['val'][-1]['roc_auc']:.3f}/{history['test'][-1]['roc_auc']:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Dynamically adjust the learning rate (for val set AUC)\n",
    "    scheduler.step(history['val'][-1]['roc_auc'])\n",
    "\n",
    "    if history['val'][-1]['roc_auc'] > best_auc:\n",
    "        best_auc = history['val'][-1]['roc_auc']\n",
    "        wait = 0\n",
    "        best_model_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early Stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Restore optimal weights\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"训练完成，最佳模型加载完毕！\")\n",
    "\n",
    "plot_metrics_curve(history, \"DualGAT-MLP\", \"outputs_DualGAT_MLP\")\n",
    "print(\"指标曲线已保存。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df9516",
   "metadata": {},
   "source": [
    "# 8.Sheet结构预测与可视化生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7d2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_data_pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_result/edge_sa\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 收集所有 sheet\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m all_sheets \u001b[38;5;241m=\u001b[39m \u001b[43mnode_data_pd\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 遍历所有 sheet，逐张绘制并保存\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sheet_idx, sheet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(all_sheets, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Sheets\u001b[39m\u001b[38;5;124m\"\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'node_data_pd' is not defined"
     ]
    }
   ],
   "source": [
    "# When running Jupyter, ensure that the memory is released after each graph is drawn, or reduce the dpi (600→300)\n",
    "import matplotlib.pyplot as plt  \n",
    "from tqdm import tqdm            # Used to display a progress bar, optional\n",
    "import gc                        # Clear Memory\n",
    "\n",
    "# Make sure the output directory exists\n",
    "output_dir = 'outputs_GSL_KAN_ADV'\n",
    "output_dir_depth = os.path.join(output_dir, 'sheet_depthvis')\n",
    "os.makedirs(output_dir_depth, exist_ok=True)\n",
    "\n",
    "output_dir_depth_pred = os.path.join(output_dir, 'sheet_depthvis_pred')\n",
    "os.makedirs(output_dir_depth_pred, exist_ok=True)\n",
    "\n",
    "output_dir_depth_sa = os.path.join(output_dir, \"sheet_depthvis_sa\")\n",
    "os.makedirs(output_dir_depth_sa, exist_ok=True)\n",
    "\n",
    "os.makedirs('output_result/edge_pred', exist_ok=True)\n",
    "os.makedirs('output_result/edge_sa', exist_ok=True)\n",
    "\n",
    "# Make sure to show progress each time through the loop\n",
    "all_sheets = node_data_pd['source'].unique()\n",
    "\n",
    "# Traverse all sheets\n",
    "for sheet_idx, sheet in enumerate(tqdm(all_sheets, desc=\"Processing Sheets\")):\n",
    "    print(f\"[INFO] 开始处理 Sheet: {sheet} ({sheet_idx + 1}/{len(all_sheets)})\")\n",
    "\n",
    "    # Get the nodes and corresponding edges of the current sheet\n",
    "    nd = node_data_pd[node_data_pd['source'] == sheet]\n",
    "    node_ids = nd['global_id'].tolist()\n",
    "\n",
    "    real_sheet_edges = []\n",
    "    for a in node_ids:\n",
    "        for b in node_ids:\n",
    "            if a < b and tuple(sorted((a, b))) in all_real_edges:\n",
    "                real_sheet_edges.append([a, b])\n",
    "    \n",
    "    # Actual levels and depth\n",
    "    real_layers = get_true_layers(node_ids, real_sheet_edges, nd)\n",
    "    real_depths = {n: d for d, layer in enumerate(real_layers) for n in layer}\n",
    "\n",
    "    # 1. Save the real structure diagram\n",
    "    try:\n",
    "        plot_depth_graph(\n",
    "            nd, real_sheet_edges, real_depths,\n",
    "            id_col='ID', name_col='Name',\n",
    "            save_path=os.path.join(output_dir_depth, f'{sheet.replace(\".\", \"_\").replace(\"/\", \"_\")}_real.png'),\n",
    "            color='#f7941d', title=f'{sheet} 实际结构'\n",
    "        )\n",
    "        print(f\" - [INFO] 真实结构图已保存: {sheet}_real.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 生成真实结构图时出错: {e}\")\n",
    "\n",
    "    # Inference prediction\n",
    "    prob_mat, idxmap = infer_sheet_prob_matrix(model, features, edge_index, node_ids)\n",
    "\n",
    "    # Get the root node\n",
    "    root_nodes = get_root_layer_nodes_for_sheet(nd, node_ids)\n",
    "\n",
    "    # Generate prediction levels and depth\n",
    "    pred_layers = pred_structure_deep_layers(prob_mat, node_ids, nd, real_layers, root_nodes)\n",
    "    pred_depths = {n: d for d, layer in enumerate(pred_layers) for n in layer}\n",
    "\n",
    "    # Prediction Edge\n",
    "    pred_edges = make_pred_edges_by_layers(prob_mat, pred_layers, pred_prob_thresh=0.5)\n",
    "\n",
    "    # 2. Save the prediction structure diagram\n",
    "    try:\n",
    "        plot_depth_graph(\n",
    "            nd, pred_edges, pred_depths,\n",
    "            id_col='ID', name_col='Name',\n",
    "            save_path=os.path.join(output_dir_depth_pred, f'{sheet.replace(\".\", \"_\").replace(\"/\", \"_\")}_pred.png'),\n",
    "            color='green', title=f'{sheet} 预测深度结构'\n",
    "        )\n",
    "        print(f\" - [INFO] 预测深度结构图已保存: {sheet}_pred.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 生成预测深度结构图时出错: {e}\")\n",
    "\n",
    "    # Annealing optimization\n",
    "    sa_edges = depth_sa_optimize(prob_mat, node_ids, nd, real_layers, pred_edges)\n",
    "    sa_pred_layers = bfs_tree_layers_priority(sa_edges, node_ids, nd)\n",
    "    sa_depths = sa_pred_layers.copy()\n",
    "\n",
    "    # 3. Save the structure diagram after annealing optimization\n",
    "    try:\n",
    "        plot_depth_graph(\n",
    "            nd, sa_edges, sa_depths,\n",
    "            id_col='ID', name_col='Name',\n",
    "            save_path=os.path.join(output_dir_depth_sa, f'{sheet.replace(\".\", \"_\").replace(\"/\", \"_\")}_pred_sa.png'),\n",
    "            color='purple', title=f'{sheet} 退火优化结构'\n",
    "        )\n",
    "        print(f\" - [INFO] 退火优化深度结构图已保存: {sheet}_pred_sa.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 生成退火优化深度结构图时出错: {e}\")\n",
    "\n",
    "    # Save edge data to CSV\n",
    "    try:\n",
    "        save_edges(pred_edges, nd, f'output_result/edge_pred/{sheet}.csv')\n",
    "        save_edges(sa_edges, nd, f'output_result/edge_sa/{sheet}.csv')\n",
    "        print(f\" - [INFO] 边数据已保存: {sheet}.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] 保存边数据时出错: {e}\")\n",
    "\n",
    "    plt.close('all')  \n",
    "    gc.collect()      \n",
    "\n",
    "print(\"[OK] 所有 sheet 的真实/多层预测/分层退火优化结构图与指标曲线已生成\")\n",
    "print(\"[OK] edge_pred、edge_sa 的 CSV 文件已保存到 output_result\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d84f6",
   "metadata": {},
   "source": [
    "# 9.Visualizing graph structure and feature prediction (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6684e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 绘图已保存为 output_result/vis_html_pred\\化工学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\化工学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\化工学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\化工学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\化工学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\土木学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\土木学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\土木学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\土木学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\土木学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_-1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\建筑与设计学院.xlsx_6f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\机电学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\机电学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\机电学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\机电学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\机电学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\物理材料学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\物理材料学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\物理材料学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\物理材料学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\物理材料学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\环测学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\环测学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\环测学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\环测学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\环测学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\矿业学院.xlsx_6f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\管理学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\管理学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\管理学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\管理学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\管理学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\资源与地球学院.xlsx_1f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\资源与地球学院.xlsx_2f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\资源与地球学院.xlsx_3f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\资源与地球学院.xlsx_4f_pred_plotly.html\n",
      "[OK] 绘图已保存为 output_result/vis_html_pred\\资源与地球学院.xlsx_5f_pred_plotly.html\n",
      "[OK] 所有绘图已生成并保存在：output_result/vis_html_pred\n"
     ]
    }
   ],
   "source": [
    "from collections import deque, defaultdict\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def plotly_depth_graph_like_matplotlib_with_roots(\n",
    "    df, edge_list, node_data,\n",
    "    id_col='id', name_col='Name',\n",
    "    feature_names=None,\n",
    "    save_path=\"depth_pred_interactive.html\",\n",
    "    node_color='#21cbbb',\n",
    "    edge_color='gray',\n",
    "    title='预测结构交互图'\n",
    "):\n",
    "    def get_root_layer_nodes_for_sheet(node_data, node_ids):\n",
    "        df_filtered = node_data[node_data[id_col].isin(node_ids)]\n",
    "        for tag in ['EN', 'LT', 'DT']:\n",
    "            filtered = df_filtered[df_filtered[name_col] == tag][id_col].tolist()\n",
    "            if filtered:\n",
    "                return filtered\n",
    "        return [node_ids[0]]\n",
    "\n",
    "    def bfs_tree_layers_priority(edges, node_ids, root_nodes):\n",
    "        edge_set = set((min(a, b), max(a, b)) for a, b in edges)\n",
    "        layers = {nid: 0 for nid in root_nodes}\n",
    "        queue = deque(root_nodes)\n",
    "        visited = set(root_nodes)\n",
    "\n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            for neighbor in node_ids:\n",
    "                if neighbor not in visited and (min(current, neighbor), max(current, neighbor)) in edge_set:\n",
    "                    layers[neighbor] = layers[current] + 1\n",
    "                    visited.add(neighbor)\n",
    "                    queue.append(neighbor)\n",
    "\n",
    "        return layers\n",
    "\n",
    "    node_ids = list(df[id_col].values)\n",
    "    root_nodes = get_root_layer_nodes_for_sheet(node_data, node_ids)\n",
    "    node_depths = bfs_tree_layers_priority(edge_list, node_ids, root_nodes)\n",
    "\n",
    "    names = dict(zip(df[id_col], df[name_col]))\n",
    "    depth_layers = defaultdict(list)\n",
    "    for node, depth in node_depths.items():\n",
    "        depth_layers[depth].append(node)\n",
    "\n",
    "    max_x_gap = 1.1\n",
    "    max_layer_width = max(len(nodes) for nodes in depth_layers.values()) * max_x_gap\n",
    "\n",
    "    y_gap = 2.7 if len(depth_layers) <= 10 else 1.5\n",
    "    positions = {}\n",
    "    for depth, layer_nodes in depth_layers.items():\n",
    "        for i, node_id in enumerate(sorted(layer_nodes, key=lambda x: names[x])):\n",
    "            positions[node_id] = (i * max_x_gap, depth * y_gap)\n",
    "\n",
    "    # Create node hover information\n",
    "    def format_float_safe(value):\n",
    "        try:\n",
    "            if pd.isna(value):\n",
    "                return ''\n",
    "            return f\"{float(value):.2f}\"\n",
    "        except:\n",
    "            return str(value)\n",
    "\n",
    "    hover_texts = {}\n",
    "    ZERO_EPS = 1e-6\n",
    "    def safe_parse(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return np.nan   # or None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        node_id = row[id_col]\n",
    "\n",
    "        column_labels = [\"Feature\", \"Real\", \"Pred\", \"Error\"]   \n",
    "        actual_values = [format_float_safe(row[f]) for f in feature_names]\n",
    "        predicted_values = [format_float_safe(row[f\"pred_{f}\"]) for f in feature_names]\n",
    "\n",
    "        error_values = []\n",
    "        for f in feature_names:\n",
    "            v_true = safe_parse(row[f])\n",
    "            v_pred = safe_parse(row[f\"pred_{f}\"])\n",
    "            if pd.isna(v_true) or pd.isna(v_pred):\n",
    "                error_values.append('')\n",
    "            elif abs(v_true) < ZERO_EPS:\n",
    "                error_values.append(f\"{abs(v_pred - v_true):.4f}*\")\n",
    "            else:\n",
    "                error_values.append(f\"{abs((v_pred - v_true) / v_true * 100):.1f}%\")\n",
    "\n",
    "        data_rows = list(zip(feature_names, actual_values, predicted_values, error_values))\n",
    "        table_cols = list(zip(*([column_labels] + data_rows)))  \n",
    "        col_widths = [max(len(str(item)) for item in col) for col in table_cols]\n",
    "\n",
    "        def format_row(row):\n",
    "            return \" | \".join(str(val).ljust(width) for val, width in zip(row, col_widths))\n",
    "\n",
    "        table = \"<br>\".join(format_row(row) for row in [column_labels] + data_rows)\n",
    "        n_name = str(row[name_col])\n",
    "        hover_texts[node_id] = f\"<b>{n_name}</b><br><span style='font-family:monospace;white-space:pre'>{table}</span>\"\n",
    "\n",
    "    edge_traces = []\n",
    "    for src, dst in edge_list:\n",
    "        if src in positions and dst in positions:\n",
    "            x0, y0 = positions[src]\n",
    "            x1, y1 = positions[dst]\n",
    "            edge_traces.append(go.Scatter(\n",
    "                x=[x0, x1], y=[y0, y1],\n",
    "                mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='skip',\n",
    "                showlegend=False\n",
    "            ))\n",
    "\n",
    "    node_traces = []\n",
    "    for node_id, (x, y) in positions.items():\n",
    "        node_label = names[node_id]\n",
    "        hover_info = hover_texts.get(node_id, node_label)\n",
    "        node_traces.append(go.Scatter(\n",
    "            x=[x], y=[y],\n",
    "            mode='markers+text',\n",
    "            text=node_label,\n",
    "            textposition='top center',\n",
    "            hovertext=hover_info,\n",
    "            marker=dict(size=35, color=node_color, line=dict(width=2, color='#0c515b')),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    depth_lines = []\n",
    "    for depth in range(len(depth_layers)):\n",
    "        y = depth * y_gap\n",
    "        x_min = -1\n",
    "        x_max = max_layer_width + 1\n",
    "        depth_lines.append(go.Scatter(\n",
    "            x=[x_min, x_max], y=[y, y],\n",
    "            mode='lines',\n",
    "            line=dict(color='lightgray', dash='dot'),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False\n",
    "        ))\n",
    "        depth_lines.append(go.Scatter(\n",
    "            x=[x_min - 1], y=[y],\n",
    "            mode='text',\n",
    "            text=[f\"Depth: {depth}\"],\n",
    "            textfont=dict(size=12, color='gray'),\n",
    "            hoverinfo='skip',\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    fig = go.Figure(data=depth_lines + edge_traces + node_traces)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(visible=False, zeroline=False, showgrid=False),\n",
    "        yaxis=dict(visible=False, zeroline=False, showgrid=False),\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(l=20, r=20, t=50, b=20)\n",
    "    )\n",
    "\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"[OK] 绘图已保存为 {save_path}\")\n",
    "\n",
    "# The main code traverses the node and edge data and draws\n",
    "node_dir = \"output_result/node_pred\"  # Node file path\n",
    "edge_dir = \"output_result/edge_pred\"  # Edge file path\n",
    "save_dir = \"output_result/vis_html_pred\"  # Save the HTML file path of the drawing\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "node_files = sorted(glob(os.path.join(node_dir, \"*.csv\")))  # Node file\n",
    "edge_files = sorted(glob(os.path.join(edge_dir, \"*.csv\")))  # Edge file\n",
    "\n",
    "# Make sure the number of files matches\n",
    "if len(node_files) != len(edge_files):\n",
    "    raise ValueError(f\"[Error] 节点文件数 ({len(node_files)}) 和边文件数 ({len(edge_files)}) 不匹配！\")\n",
    "\n",
    "for node_file, edge_file in zip(node_files, edge_files):\n",
    "    sheet_name = os.path.splitext(os.path.basename(node_file))[0]  # The name of the current sheet\n",
    "    node_data = pd.read_csv(node_file)\n",
    "    edge_data = pd.read_csv(edge_file)\n",
    "\n",
    "    if not set(['src_id', 'dst_id']).issubset(edge_data.columns):\n",
    "        raise ValueError(f\"[Error] 边文件 {edge_file} 缺少必要列 'src_id', 'dst_id'\")\n",
    "    edge_list = list(zip(edge_data['src_id'], edge_data['dst_id']))\n",
    "\n",
    "    # Automatically filter all feature names\n",
    "    feature_names = [col for col in node_data.columns if col in [\n",
    "        'ITG', 'BTW', 'CTR', 'ETR', 'S', 'L', 'WA', 'WFR', 'WWR', 'DN',\n",
    "        'WN', 'DS', 'WS', 'H', 'V', 'a', 'b', 'OH', 'IH', 'CN', 'CA'\n",
    "    ]]\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f\"{sheet_name}_pred_plotly.html\")\n",
    "    plotly_depth_graph_like_matplotlib_with_roots(\n",
    "        df=node_data,\n",
    "        edge_list=edge_list,\n",
    "        node_data=node_data,\n",
    "        id_col='id',         \n",
    "        name_col='name',\n",
    "        feature_names=feature_names,\n",
    "        save_path=save_path,\n",
    "        title=f'{sheet_name} 图结构及特征预测'\n",
    "    )\n",
    "\n",
    "print(f\"[OK] 所有绘图已生成并保存在：{save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aee6ae",
   "metadata": {},
   "source": [
    "# 10.Visualizing graph structure and feature prediction （SA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\化工学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\化工学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\化工学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\化工学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\化工学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\土木学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\土木学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\土木学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\土木学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\土木学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_-1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\建筑与设计学院.xlsx_6f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\机电学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\机电学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\机电学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\机电学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\机电学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\物理材料学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\物理材料学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\物理材料学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\物理材料学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\物理材料学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\环测学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\环测学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\环测学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\环测学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\环测学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\矿业学院.xlsx_6f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\管理学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\管理学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\管理学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\管理学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\管理学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\资源与地球学院.xlsx_1f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\资源与地球学院.xlsx_2f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\资源与地球学院.xlsx_3f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\资源与地球学院.xlsx_4f_sa_plotly.html\n",
      "[SA] 绘图已保存 output_result/vis_html_pred_sa\\资源与地球学院.xlsx_5f_sa_plotly.html\n",
      "[SA] 所有绘图已生成保存在：output_result/vis_html_pred_sa\n"
     ]
    }
   ],
   "source": [
    "from collections import deque, defaultdict\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "def get_layer_groups(edges, node_ids, root_nodes):\n",
    "    edge_set = set((min(a, b), max(a, b)) for a, b in edges)\n",
    "    layers = {nid: 0 for nid in root_nodes}\n",
    "    queue = deque(root_nodes)\n",
    "    visited = set(root_nodes)\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        for neighbor in node_ids:\n",
    "            if neighbor not in visited and (min(current, neighbor), max(current, neighbor)) in edge_set:\n",
    "                layers[neighbor] = layers[current] + 1\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    layer_to_nodes = defaultdict(list)\n",
    "    for nid, d in layers.items():\n",
    "        layer_to_nodes[d].append(nid)\n",
    "    return layers, layer_to_nodes\n",
    "\n",
    "# Calculate the overlap ratio of two set edges (the larger the better)\n",
    "def overlap_ratio(e_new, e_orig):\n",
    "    if not e_new:\n",
    "        return 0\n",
    "    intersect = len(e_new.intersection(e_orig))\n",
    "    return intersect / len(e_new)\n",
    "\n",
    "def sa_modify_edges(edges_cur, pre_nodes, nxt_nodes, orig_edges_set):\n",
    "    edges_list = list(edges_cur)\n",
    "    if np.random.rand() < 0.5 and edges_list:\n",
    "        idx = np.random.randint(len(edges_list))\n",
    "        u, v = edges_list[idx]\n",
    "        count_u = sum(1 for e in edges_cur if u in e)\n",
    "        count_v = sum(1 for e in edges_cur if v in e)\n",
    "        if (count_u > 1) and (count_v > 1):\n",
    "            new_edges = edges_cur.copy()\n",
    "            new_edges.remove((u, v))\n",
    "            return new_edges\n",
    "\n",
    "    candidate_edges = []\n",
    "    for u in pre_nodes:\n",
    "        for v in nxt_nodes:\n",
    "            e = (min(u, v), max(u, v))\n",
    "            if e not in edges_cur:\n",
    "                candidate_edges.append(e)\n",
    "    if candidate_edges:\n",
    "        new_edge = candidate_edges[np.random.randint(len(candidate_edges))]\n",
    "        new_edges = edges_cur.copy()\n",
    "        new_edges.add(new_edge)\n",
    "        return new_edges\n",
    "    return edges_cur\n",
    "\n",
    "# SA core optimization, optimizing the overlap and connectivity between the two layers\n",
    "def sa_optimize_layer_edges(pre_nodes, nxt_nodes, orig_edges_set, max_iter=3000, init_temp=1.0, final_temp=0.01):\n",
    "    edges_cur = set(e for e in orig_edges_set if (e[0] in pre_nodes and e[1] in nxt_nodes) or (e[1] in pre_nodes and e[0] in nxt_nodes))\n",
    "    def has_isolated(edges, pre_n, nxt_n):\n",
    "        linked_pre = set()\n",
    "        linked_nxt = set()\n",
    "        for u,v in edges:\n",
    "            if u in pre_n:\n",
    "                linked_pre.add(u)\n",
    "            if v in pre_n:\n",
    "                linked_pre.add(v)\n",
    "            if u in nxt_n:\n",
    "                linked_nxt.add(u)\n",
    "            if v in nxt_n:\n",
    "                linked_nxt.add(v)\n",
    "        isolated_pre = set(pre_n) - linked_pre\n",
    "        isolated_nxt = set(nxt_n) - linked_nxt\n",
    "        return isolated_pre, isolated_nxt\n",
    "    isolated_pre, isolated_nxt = has_isolated(edges_cur, pre_nodes, nxt_nodes)\n",
    "    for u in isolated_pre:\n",
    "        edges_cur.add((min(u, nxt_nodes[0]), max(u, nxt_nodes[0])))\n",
    "    for v in isolated_nxt:\n",
    "        edges_cur.add((min(pre_nodes[0], v), max(pre_nodes[0], v)))\n",
    "    temp = init_temp\n",
    "    alpha = (final_temp/init_temp)**(1/(max_iter-1))\n",
    "    best_edges = edges_cur.copy()\n",
    "    best_score = overlap_ratio(edges_cur, orig_edges_set)\n",
    "    for i in range(max_iter):\n",
    "        isolated_pre, isolated_nxt = has_isolated(edges_new, pre_nodes, nxt_nodes)\n",
    "        if isolated_pre or isolated_nxt:\n",
    "            continue\n",
    "        score_new = overlap_ratio(edges_new, orig_edges_set)\n",
    "        score_diff = score_new - overlap_ratio(edges_cur, orig_edges_set)\n",
    "        accept = False\n",
    "        if score_diff >= 0:\n",
    "            accept = True\n",
    "        else:\n",
    "            p = np.exp(score_diff / temp)\n",
    "            if np.random.rand() < p:\n",
    "                accept = True\n",
    "        if accept:\n",
    "            edges_cur = edges_new\n",
    "            if score_new > best_score:\n",
    "                best_score = score_new\n",
    "                best_edges = edges_new\n",
    "        temp *= alpha\n",
    "    return best_edges\n",
    "\n",
    "\n",
    "def full_depth_graph_with_sa_edges(\n",
    "    df, orig_edge_list, node_data, id_col='id', name_col='Name',\n",
    "    feature_names=None, save_path=\"depth_pred_interactive_sa.html\",\n",
    "    node_color='#21cbbb', edge_color='gray', title='预测结构交互图',\n",
    "    rand_seed=42, sa_iter=3000\n",
    "):\n",
    "    np.random.seed(rand_seed)\n",
    "\n",
    "    def get_root_layer_nodes_for_sheet(node_data, node_ids):\n",
    "        for tag in ['EN', 'DT', 'LT']:\n",
    "            filtered = node_data[\n",
    "                node_data[id_col].isin(node_ids) & node_data[name_col].str.contains(tag, na=False)\n",
    "            ][id_col].tolist()\n",
    "            if filtered:\n",
    "                return filtered\n",
    "        return [node_ids[0]]\n",
    "\n",
    "    node_ids = list(df[id_col].values)\n",
    "    root_nodes = get_root_layer_nodes_for_sheet(node_data, node_ids)\n",
    "    layers, layer_to_nodes = get_layer_groups(orig_edge_list, node_ids, root_nodes)\n",
    "    orig_edges_set = set((min(u,v), max(u,v)) for u,v in orig_edge_list)\n",
    "\n",
    "    new_edges_set = set()\n",
    "    depth_list = sorted(layer_to_nodes.keys())\n",
    "    # For each pair of adjacent layers, use SA to optimize the edge set\n",
    "    for d, dn in zip(depth_list[:-1], depth_list[1:]):\n",
    "        pre_n = layer_to_nodes[d]\n",
    "        nxt_n = layer_to_nodes[dn]\n",
    "        if pre_n and nxt_n:\n",
    "            es = sa_optimize_layer_edges(pre_n, nxt_n, orig_edges_set, max_iter=sa_iter)\n",
    "            new_edges_set |= es\n",
    "\n",
    "    # Add non-adjacent layer original edges to ensure the overall structure\n",
    "    for u, v in orig_edges_set:\n",
    "        depth_u = layers.get(u, -1)\n",
    "        depth_v = layers.get(v, -1)\n",
    "        if abs(depth_u - depth_v) != 1:\n",
    "            new_edges_set.add((min(u,v), max(u,v)))\n",
    "\n",
    "    linked_nodes = set()\n",
    "    for u,v in new_edges_set:\n",
    "        linked_nodes.add(u); linked_nodes.add(v)\n",
    "    for nid in node_ids:\n",
    "        if nid not in linked_nodes and nid != root_nodes[0]:\n",
    "            new_edges_set.add((min(nid, node_ids[0]), max(nid, node_ids[0])))\n",
    "\n",
    "    # Node location\n",
    "    max_x_gap = 1.1\n",
    "    y_gap = 2.7 if len(layer_to_nodes) <= 10 else 1.5\n",
    "    max_layer_width = max(len(nodes) for nodes in layer_to_nodes.values()) * max_x_gap\n",
    "    positions = {}\n",
    "    names = dict(zip(df[id_col], df[name_col]))\n",
    "    for depth in sorted(layer_to_nodes.keys()):\n",
    "        layer_nodes = sorted(layer_to_nodes[depth], key=lambda x: (str(names.get(x, '')), x))\n",
    "        for i, node_id in enumerate(layer_nodes):\n",
    "            positions[node_id] = (i * max_x_gap, depth * y_gap)\n",
    "    # hover TXT\n",
    "    def format_float_safe(value):\n",
    "        try:\n",
    "            if pd.isna(value): return ''\n",
    "            return f\"{float(value):.2f}\"\n",
    "        except:\n",
    "            return str(value)\n",
    "    hover_texts = {}\n",
    "    ZERO_EPS = 1e-6\n",
    "    def safe_parse(x):\n",
    "        try: return float(x)\n",
    "        except: return np.nan\n",
    "    for _, row in df.iterrows():\n",
    "        node_id = row[id_col]\n",
    "        column_labels = [\"Feature\", \"Real\", \"Pred\", \"Error\"]\n",
    "        actual_values = [format_float_safe(row[f]) for f in feature_names]\n",
    "        predicted_values = [format_float_safe(row[f\"pred_{f}\"]) for f in feature_names]\n",
    "        error_values = []\n",
    "        for f in feature_names:\n",
    "            v_true = safe_parse(row[f])\n",
    "            v_pred = safe_parse(row[f\"pred_{f}\"])\n",
    "            if pd.isna(v_true) or pd.isna(v_pred): error_values.append('')\n",
    "            elif abs(v_true) < ZERO_EPS: error_values.append(f\"{abs(v_pred - v_true):.4f}*\")\n",
    "            else: error_values.append(f\"{abs((v_pred - v_true) / v_true * 100):.1f}%\")\n",
    "        data_rows = list(zip(feature_names, actual_values, predicted_values, error_values))\n",
    "        table_cols = list(zip(*([column_labels] + data_rows)))\n",
    "        col_widths = [max(len(str(item)) for item in col) for col in table_cols]\n",
    "        def format_row(row):\n",
    "            return \" | \".join(str(val).ljust(width) for val, width in zip(row, col_widths))\n",
    "        table = \"<br>\".join(format_row(row) for row in [column_labels] + data_rows)\n",
    "        n_name = str(row[name_col])\n",
    "        hover_texts[node_id] = f\"<b>{n_name}</b><br><span style='font-family:monospace;white-space:pre'>{table}</span>\"\n",
    "    # Draw Edges\n",
    "    edge_traces = []\n",
    "    for u,v in new_edges_set:\n",
    "        if u in positions and v in positions:\n",
    "            x0,y0 = positions[u]\n",
    "            x1,y1 = positions[v]\n",
    "            edge_traces.append(go.Scatter(\n",
    "                x=[x0,x1], y=[y0,y1], mode='lines',\n",
    "                line=dict(color=edge_color, width=2),\n",
    "                hoverinfo='skip', showlegend=False))\n",
    "    # Drawing Nodes\n",
    "    node_traces = []\n",
    "    for node_id,(x,y) in positions.items():\n",
    "        node_label = names[node_id]\n",
    "        hover_info = hover_texts.get(node_id, node_label)\n",
    "        node_traces.append(go.Scatter(\n",
    "            x=[x], y=[y],mode='markers+text',\n",
    "            text=node_label, textposition='top center',\n",
    "            hovertext=hover_info,\n",
    "            marker=dict(size=35, color=node_color, line=dict(width=2, color='#0c515b')),\n",
    "            showlegend=False))\n",
    "    # Horizontal depth mark\n",
    "    depth_lines = []\n",
    "    for depth in sorted(layer_to_nodes.keys()):\n",
    "        y = depth * y_gap\n",
    "        x_min = -1\n",
    "        x_max = max_layer_width + 1\n",
    "        depth_lines.append(go.Scatter(\n",
    "            x=[x_min, x_max], y=[y,y], mode='lines',\n",
    "            line=dict(color='lightgray', dash='dot'),\n",
    "            hoverinfo='skip', showlegend=False\n",
    "        ))\n",
    "        depth_lines.append(go.Scatter(\n",
    "            x=[x_min-1], y=[y], mode='text', \n",
    "            text=[f\"Depth: {depth}\"], \n",
    "            textfont=dict(size=12, color='gray'),\n",
    "            hoverinfo='skip', showlegend=False\n",
    "        ))\n",
    "    fig = go.Figure(data=depth_lines + edge_traces + node_traces)\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(visible=False, zeroline=False, showgrid=False),\n",
    "        yaxis=dict(visible=False, zeroline=False, showgrid=False),\n",
    "        plot_bgcolor='white', margin=dict(l=20, r=20, t=50, b=20)\n",
    "    )\n",
    "    fig.write_html(save_path)\n",
    "    print(f\"[SA] 绘图已保存 {save_path}\")\n",
    "\n",
    "# ===== Main program =====\n",
    "node_dir = \"output_result/node_pred\"\n",
    "edge_dir = \"output_result/edge_pred\"\n",
    "save_dir = \"output_result/vis_html_pred_sa\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "node_files = sorted(glob(os.path.join(node_dir, \"*.csv\")))\n",
    "edge_files = sorted(glob(os.path.join(edge_dir, \"*.csv\")))\n",
    "\n",
    "if len(node_files) != len(edge_files):\n",
    "    raise ValueError(\"节点文件和边文件数量不匹配！\")\n",
    "\n",
    "for node_file, edge_file in zip(node_files, edge_files):\n",
    "    sheet_name = os.path.splitext(os.path.basename(node_file))[0]\n",
    "    node_data = pd.read_csv(node_file)\n",
    "    edge_data = pd.read_csv(edge_file)\n",
    "    if not set(['src_id', 'dst_id']).issubset(edge_data.columns):\n",
    "        raise ValueError(f\"边文件{edge_file}缺少'src_id', 'dst_id'列！\")\n",
    "\n",
    "    orig_edges = list(zip(edge_data['src_id'], edge_data['dst_id']))\n",
    "    feature_names = [col for col in node_data.columns if col in [\n",
    "        'ITG', 'BTW', 'CTR', 'ETR', 'S', 'L', 'WA', 'WFR', 'WWR', 'DN',\n",
    "        'WN', 'DS', 'WS', 'H', 'V', 'a', 'b', 'OH', 'IH', 'CN', 'CA'\n",
    "    ]]\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{sheet_name}_sa_plotly.html\")\n",
    "    full_depth_graph_with_sa_edges(\n",
    "        df=node_data,\n",
    "        orig_edge_list=orig_edges,\n",
    "        node_data=node_data,\n",
    "        id_col='id',\n",
    "        name_col='name',\n",
    "        feature_names=feature_names,\n",
    "        save_path=save_path,\n",
    "        title=f\"{sheet_name} SA 优化结构预测图\",\n",
    "        rand_seed=42,\n",
    "        sa_iter=3000 # Adjustable number of simulated annealing iterations\n",
    "    )\n",
    "print(f\"[SA] 所有绘图已生成保存在：{save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
